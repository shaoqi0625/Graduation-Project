# Graduation-Project

With the rapid development of natural language processing (NLP) technology, especially the introduction of deep learning technology, text classification tasks have made remarkable progress. The rapid development of pre-trained language models has significantly improved the performance of text classification. However, these models face threats to their security and robustness in the face of well-designed text attacks. An attacker may exploit a vulnerability in the model by adding specific perturbations to deceive the model, resulting in a miscalculation or misclassification.

This paper proposes a new text attack method, which uses a gradience-based approach to generate universal trigger tokens, and uses a word-based substitution strategy to generate adversarial samples, and combines the two. In addition, it also discusses the generation of adversarial samples based on large language models, aiming to improve the accuracy and efficiency of text attacks. And enhance the deception of generating adversarial samples. In this paper, the effectiveness and universality of the proposed method are verified by attack experiments on multiple pre-trained language models and multiple data sets.

The research in this paper shows that the pre-trained language model has certain security problems in the face of text attacks in the text classification task, and improving the robustness of the model is crucial to cope with text attacks. By deeply understanding the nature of text attacks and the characteristics of pre-trained language models, we can provide new ideas and methods to improve the security and robustness of the models. This study not only provides a new method and direction for text attack research, but also contributes to the development and application of NLP technology.
