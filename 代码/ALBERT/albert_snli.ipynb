{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43a9a36-254b-496d-81ba-fafce925fcb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "import warnings\n",
    "import csv\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "global extracted_grads\n",
    "\n",
    "extracted_grads = []\n",
    "position = 1  # concatenation position\n",
    "# the concatenation position of the BERT model is after the [CLS] token\n",
    "# Random Concatenation Mode\n",
    "# position = random.randint(1,500)\n",
    "\n",
    "tokenize = AlbertTokenizer.from_pretrained(\"/root/albert\")\n",
    "Model = AlbertModel.from_pretrained(\"/root/albert\")\n",
    "\n",
    "# Load model related information\n",
    "\n",
    "# Print the number of Total Parameters\n",
    "# total = [param.nelement() for param in Model.parameters()]\n",
    "# print(f'total parameters:{format(sum(total))}\\n each layer parameters{total} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d95b9c-e8d7-4dd8-9153-f0c3d3d7c74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_data(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data_ent(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    return texts, labels\n",
    "\n",
    "def read_snli_binary_test_data_con(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def load_snli_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Constructs a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def load_snli_data_ent(train_batch_size, test_batch_iter, num_steps=500):\n",
    "    train_data = read_snli_binary_data('snli_1.0', is_train=True)\n",
    "    test_data = read_snli_binary_test_data_ent('snli_1.0', is_train=False)\n",
    "    train_encoding = tokenize(train_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    train_iter = load_snli_array(\n",
    "        (train_encoding['input_ids'], train_encoding['token_type_ids'], torch.tensor(train_data[1])),\n",
    "        train_batch_size)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "def load_snli_data_con(train_batch_size, test_batch_iter, num_steps=500):\n",
    "    train_data = read_snli_binary_data('snli_1.0', is_train=True)\n",
    "    test_data = read_snli_binary_test_data_con('snli_1.0', is_train=False)\n",
    "    train_encoding = tokenize(train_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    train_iter = load_snli_array(\n",
    "        (train_encoding['input_ids'], train_encoding['token_type_ids'], torch.tensor(train_data[1])),\n",
    "        train_batch_size)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3d127e-68b0-4d15-9045-b7a6ccc6ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n",
      "18331\n",
      "1123\n",
      "1079\n"
     ]
    }
   ],
   "source": [
    "# train_iter, test_iter = load_imdb_data(10)\n",
    "# train_iter, test_iter = load_sst_data(10)\n",
    "train_iter, test_iter_ent = load_snli_data_ent(20, 3)\n",
    "train_iter, test_iter_con = load_snli_data_con(20, 3)\n",
    "# Data preprocessing and loading\n",
    "print(\"reading data finished\\n\")\n",
    "print(len(train_iter))\n",
    "print(len(test_iter_ent))\n",
    "print(len(test_iter_con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93db2eaa-ac40-4138-aaa6-2dd1d941485c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Batch 100/18331, Loss: 0.7031\n",
      "Epoch 1/3, Batch 200/18331, Loss: 0.7025\n",
      "Epoch 1/3, Batch 300/18331, Loss: 0.7007\n",
      "Epoch 1/3, Batch 400/18331, Loss: 0.7003\n",
      "Epoch 1/3, Batch 500/18331, Loss: 0.6998\n",
      "Epoch 1/3, Batch 600/18331, Loss: 0.6994\n",
      "Epoch 1/3, Batch 700/18331, Loss: 0.6991\n",
      "Epoch 1/3, Batch 800/18331, Loss: 0.6985\n",
      "Epoch 1/3, Batch 900/18331, Loss: 0.6991\n",
      "Epoch 1/3, Batch 1000/18331, Loss: 0.6985\n",
      "Epoch 1/3, Batch 1100/18331, Loss: 0.6903\n",
      "Epoch 1/3, Batch 1200/18331, Loss: 0.6746\n",
      "Epoch 1/3, Batch 1300/18331, Loss: 0.6550\n",
      "Epoch 1/3, Batch 1400/18331, Loss: 0.6357\n",
      "Epoch 1/3, Batch 1500/18331, Loss: 0.6149\n",
      "Epoch 1/3, Batch 1600/18331, Loss: 0.5975\n",
      "Epoch 1/3, Batch 1700/18331, Loss: 0.5802\n",
      "Epoch 1/3, Batch 1800/18331, Loss: 0.5648\n",
      "Epoch 1/3, Batch 1900/18331, Loss: 0.5497\n",
      "Epoch 1/3, Batch 2000/18331, Loss: 0.5369\n",
      "Epoch 1/3, Batch 2100/18331, Loss: 0.5241\n",
      "Epoch 1/3, Batch 2200/18331, Loss: 0.5135\n",
      "Epoch 1/3, Batch 2300/18331, Loss: 0.5034\n",
      "Epoch 1/3, Batch 2400/18331, Loss: 0.4935\n",
      "Epoch 1/3, Batch 2500/18331, Loss: 0.4837\n",
      "Epoch 1/3, Batch 2600/18331, Loss: 0.4752\n",
      "Epoch 1/3, Batch 2700/18331, Loss: 0.4668\n",
      "Epoch 1/3, Batch 2800/18331, Loss: 0.4594\n",
      "Epoch 1/3, Batch 2900/18331, Loss: 0.4520\n",
      "Epoch 1/3, Batch 3000/18331, Loss: 0.4454\n",
      "Epoch 1/3, Batch 3100/18331, Loss: 0.4393\n",
      "Epoch 1/3, Batch 3200/18331, Loss: 0.4331\n",
      "Epoch 1/3, Batch 3300/18331, Loss: 0.4267\n",
      "Epoch 1/3, Batch 3400/18331, Loss: 0.4213\n",
      "Epoch 1/3, Batch 3500/18331, Loss: 0.4158\n",
      "Epoch 1/3, Batch 3600/18331, Loss: 0.4108\n",
      "Epoch 1/3, Batch 3700/18331, Loss: 0.4054\n",
      "Epoch 1/3, Batch 3800/18331, Loss: 0.4005\n",
      "Epoch 1/3, Batch 3900/18331, Loss: 0.3960\n",
      "Epoch 1/3, Batch 4000/18331, Loss: 0.3915\n",
      "Epoch 1/3, Batch 4100/18331, Loss: 0.3878\n",
      "Epoch 1/3, Batch 4200/18331, Loss: 0.3837\n",
      "Epoch 1/3, Batch 4300/18331, Loss: 0.3798\n",
      "Epoch 1/3, Batch 4400/18331, Loss: 0.3767\n",
      "Epoch 1/3, Batch 4500/18331, Loss: 0.3734\n",
      "Epoch 1/3, Batch 4600/18331, Loss: 0.3700\n",
      "Epoch 1/3, Batch 4700/18331, Loss: 0.3667\n",
      "Epoch 1/3, Batch 4800/18331, Loss: 0.3637\n",
      "Epoch 1/3, Batch 4900/18331, Loss: 0.3611\n",
      "Epoch 1/3, Batch 5000/18331, Loss: 0.3579\n",
      "Epoch 1/3, Batch 5100/18331, Loss: 0.3551\n",
      "Epoch 1/3, Batch 5200/18331, Loss: 0.3527\n",
      "Epoch 1/3, Batch 5300/18331, Loss: 0.3499\n",
      "Epoch 1/3, Batch 5400/18331, Loss: 0.3473\n",
      "Epoch 1/3, Batch 5500/18331, Loss: 0.3449\n",
      "Epoch 1/3, Batch 5600/18331, Loss: 0.3427\n",
      "Epoch 1/3, Batch 5700/18331, Loss: 0.3406\n",
      "Epoch 1/3, Batch 5800/18331, Loss: 0.3383\n",
      "Epoch 1/3, Batch 5900/18331, Loss: 0.3362\n",
      "Epoch 1/3, Batch 6000/18331, Loss: 0.3344\n",
      "Epoch 1/3, Batch 6100/18331, Loss: 0.3321\n",
      "Epoch 1/3, Batch 6200/18331, Loss: 0.3298\n",
      "Epoch 1/3, Batch 6300/18331, Loss: 0.3277\n",
      "Epoch 1/3, Batch 6400/18331, Loss: 0.3258\n",
      "Epoch 1/3, Batch 6500/18331, Loss: 0.3239\n",
      "Epoch 1/3, Batch 6600/18331, Loss: 0.3221\n",
      "Epoch 1/3, Batch 6700/18331, Loss: 0.3202\n",
      "Epoch 1/3, Batch 6800/18331, Loss: 0.3190\n",
      "Epoch 1/3, Batch 6900/18331, Loss: 0.3176\n",
      "Epoch 1/3, Batch 7000/18331, Loss: 0.3160\n",
      "Epoch 1/3, Batch 7100/18331, Loss: 0.3147\n",
      "Epoch 1/3, Batch 7200/18331, Loss: 0.3128\n",
      "Epoch 1/3, Batch 7300/18331, Loss: 0.3111\n",
      "Epoch 1/3, Batch 7400/18331, Loss: 0.3094\n",
      "Epoch 1/3, Batch 7500/18331, Loss: 0.3080\n",
      "Epoch 1/3, Batch 7600/18331, Loss: 0.3066\n",
      "Epoch 1/3, Batch 7700/18331, Loss: 0.3053\n",
      "Epoch 1/3, Batch 7800/18331, Loss: 0.3038\n",
      "Epoch 1/3, Batch 7900/18331, Loss: 0.3025\n",
      "Epoch 1/3, Batch 8000/18331, Loss: 0.3010\n",
      "Epoch 1/3, Batch 8100/18331, Loss: 0.2994\n",
      "Epoch 1/3, Batch 8200/18331, Loss: 0.2980\n",
      "Epoch 1/3, Batch 8300/18331, Loss: 0.2967\n",
      "Epoch 1/3, Batch 8400/18331, Loss: 0.2956\n",
      "Epoch 1/3, Batch 8500/18331, Loss: 0.2946\n",
      "Epoch 1/3, Batch 8600/18331, Loss: 0.2935\n",
      "Epoch 1/3, Batch 8700/18331, Loss: 0.2924\n",
      "Epoch 1/3, Batch 8800/18331, Loss: 0.2915\n",
      "Epoch 1/3, Batch 8900/18331, Loss: 0.2902\n",
      "Epoch 1/3, Batch 9000/18331, Loss: 0.2894\n",
      "Epoch 1/3, Batch 9100/18331, Loss: 0.2882\n",
      "Epoch 1/3, Batch 9200/18331, Loss: 0.2872\n",
      "Epoch 1/3, Batch 9300/18331, Loss: 0.2862\n",
      "Epoch 1/3, Batch 9400/18331, Loss: 0.2852\n",
      "Epoch 1/3, Batch 9500/18331, Loss: 0.2841\n",
      "Epoch 1/3, Batch 9600/18331, Loss: 0.2831\n",
      "Epoch 1/3, Batch 9700/18331, Loss: 0.2820\n",
      "Epoch 1/3, Batch 9800/18331, Loss: 0.2811\n",
      "Epoch 1/3, Batch 9900/18331, Loss: 0.2799\n",
      "Epoch 1/3, Batch 10000/18331, Loss: 0.2789\n",
      "Epoch 1/3, Batch 10100/18331, Loss: 0.2781\n",
      "Epoch 1/3, Batch 10200/18331, Loss: 0.2771\n",
      "Epoch 1/3, Batch 10300/18331, Loss: 0.2762\n",
      "Epoch 1/3, Batch 10400/18331, Loss: 0.2752\n",
      "Epoch 1/3, Batch 10500/18331, Loss: 0.2747\n",
      "Epoch 1/3, Batch 10600/18331, Loss: 0.2736\n",
      "Epoch 1/3, Batch 10700/18331, Loss: 0.2726\n",
      "Epoch 1/3, Batch 10800/18331, Loss: 0.2718\n",
      "Epoch 1/3, Batch 10900/18331, Loss: 0.2709\n",
      "Epoch 1/3, Batch 11000/18331, Loss: 0.2700\n",
      "Epoch 1/3, Batch 11100/18331, Loss: 0.2693\n",
      "Epoch 1/3, Batch 11200/18331, Loss: 0.2686\n",
      "Epoch 1/3, Batch 11300/18331, Loss: 0.2678\n",
      "Epoch 1/3, Batch 11400/18331, Loss: 0.2670\n",
      "Epoch 1/3, Batch 11500/18331, Loss: 0.2662\n",
      "Epoch 1/3, Batch 11600/18331, Loss: 0.2656\n",
      "Epoch 1/3, Batch 11700/18331, Loss: 0.2649\n",
      "Epoch 1/3, Batch 11800/18331, Loss: 0.2641\n",
      "Epoch 1/3, Batch 11900/18331, Loss: 0.2637\n",
      "Epoch 1/3, Batch 12000/18331, Loss: 0.2631\n",
      "Epoch 1/3, Batch 12100/18331, Loss: 0.2624\n",
      "Epoch 1/3, Batch 12200/18331, Loss: 0.2619\n",
      "Epoch 1/3, Batch 12300/18331, Loss: 0.2612\n",
      "Epoch 1/3, Batch 12400/18331, Loss: 0.2605\n",
      "Epoch 1/3, Batch 12500/18331, Loss: 0.2596\n",
      "Epoch 1/3, Batch 12600/18331, Loss: 0.2588\n",
      "Epoch 1/3, Batch 12700/18331, Loss: 0.2581\n",
      "Epoch 1/3, Batch 12800/18331, Loss: 0.2574\n",
      "Epoch 1/3, Batch 12900/18331, Loss: 0.2569\n",
      "Epoch 1/3, Batch 13000/18331, Loss: 0.2563\n",
      "Epoch 1/3, Batch 13100/18331, Loss: 0.2556\n",
      "Epoch 1/3, Batch 13200/18331, Loss: 0.2548\n",
      "Epoch 1/3, Batch 13300/18331, Loss: 0.2542\n",
      "Epoch 1/3, Batch 13400/18331, Loss: 0.2535\n",
      "Epoch 1/3, Batch 13500/18331, Loss: 0.2530\n",
      "Epoch 1/3, Batch 13600/18331, Loss: 0.2523\n",
      "Epoch 1/3, Batch 13700/18331, Loss: 0.2518\n",
      "Epoch 1/3, Batch 13800/18331, Loss: 0.2512\n",
      "Epoch 1/3, Batch 13900/18331, Loss: 0.2507\n",
      "Epoch 1/3, Batch 14000/18331, Loss: 0.2500\n",
      "Epoch 1/3, Batch 14100/18331, Loss: 0.2495\n",
      "Epoch 1/3, Batch 14200/18331, Loss: 0.2489\n",
      "Epoch 1/3, Batch 14300/18331, Loss: 0.2484\n",
      "Epoch 1/3, Batch 14400/18331, Loss: 0.2479\n",
      "Epoch 1/3, Batch 14500/18331, Loss: 0.2473\n",
      "Epoch 1/3, Batch 14600/18331, Loss: 0.2467\n",
      "Epoch 1/3, Batch 14700/18331, Loss: 0.2460\n",
      "Epoch 1/3, Batch 14800/18331, Loss: 0.2455\n",
      "Epoch 1/3, Batch 14900/18331, Loss: 0.2450\n",
      "Epoch 1/3, Batch 15000/18331, Loss: 0.2445\n",
      "Epoch 1/3, Batch 15100/18331, Loss: 0.2441\n",
      "Epoch 1/3, Batch 15200/18331, Loss: 0.2435\n",
      "Epoch 1/3, Batch 15300/18331, Loss: 0.2430\n",
      "Epoch 1/3, Batch 15400/18331, Loss: 0.2426\n",
      "Epoch 1/3, Batch 15500/18331, Loss: 0.2421\n",
      "Epoch 1/3, Batch 15600/18331, Loss: 0.2416\n",
      "Epoch 1/3, Batch 15700/18331, Loss: 0.2411\n",
      "Epoch 1/3, Batch 15800/18331, Loss: 0.2407\n",
      "Epoch 1/3, Batch 15900/18331, Loss: 0.2403\n",
      "Epoch 1/3, Batch 16000/18331, Loss: 0.2398\n",
      "Epoch 1/3, Batch 16100/18331, Loss: 0.2394\n",
      "Epoch 1/3, Batch 16200/18331, Loss: 0.2390\n",
      "Epoch 1/3, Batch 16300/18331, Loss: 0.2385\n",
      "Epoch 1/3, Batch 16400/18331, Loss: 0.2381\n",
      "Epoch 1/3, Batch 16500/18331, Loss: 0.2377\n",
      "Epoch 1/3, Batch 16600/18331, Loss: 0.2373\n",
      "Epoch 1/3, Batch 16700/18331, Loss: 0.2371\n",
      "Epoch 1/3, Batch 16800/18331, Loss: 0.2366\n",
      "Epoch 1/3, Batch 16900/18331, Loss: 0.2362\n",
      "Epoch 1/3, Batch 17000/18331, Loss: 0.2357\n",
      "Epoch 1/3, Batch 17100/18331, Loss: 0.2353\n",
      "Epoch 1/3, Batch 17200/18331, Loss: 0.2349\n",
      "Epoch 1/3, Batch 17300/18331, Loss: 0.2346\n",
      "Epoch 1/3, Batch 17400/18331, Loss: 0.2341\n",
      "Epoch 1/3, Batch 17500/18331, Loss: 0.2335\n",
      "Epoch 1/3, Batch 17600/18331, Loss: 0.2330\n",
      "Epoch 1/3, Batch 17700/18331, Loss: 0.2326\n",
      "Epoch 1/3, Batch 17800/18331, Loss: 0.2321\n",
      "Epoch 1/3, Batch 17900/18331, Loss: 0.2318\n",
      "Epoch 1/3, Batch 18000/18331, Loss: 0.2313\n",
      "Epoch 1/3, Batch 18100/18331, Loss: 0.2309\n",
      "Epoch 1/3, Batch 18200/18331, Loss: 0.2305\n",
      "Epoch 1/3, Batch 18300/18331, Loss: 0.2302\n",
      "Epoch 2/3, Batch 100/18331, Loss: 0.1391\n",
      "Epoch 2/3, Batch 200/18331, Loss: 0.1370\n",
      "Epoch 2/3, Batch 300/18331, Loss: 0.1407\n",
      "Epoch 2/3, Batch 400/18331, Loss: 0.1405\n",
      "Epoch 2/3, Batch 500/18331, Loss: 0.1403\n",
      "Epoch 2/3, Batch 600/18331, Loss: 0.1421\n",
      "Epoch 2/3, Batch 700/18331, Loss: 0.1410\n",
      "Epoch 2/3, Batch 800/18331, Loss: 0.1424\n",
      "Epoch 2/3, Batch 900/18331, Loss: 0.1425\n",
      "Epoch 2/3, Batch 1000/18331, Loss: 0.1422\n",
      "Epoch 2/3, Batch 1100/18331, Loss: 0.1439\n",
      "Epoch 2/3, Batch 1200/18331, Loss: 0.1434\n",
      "Epoch 2/3, Batch 1300/18331, Loss: 0.1441\n",
      "Epoch 2/3, Batch 1400/18331, Loss: 0.1449\n",
      "Epoch 2/3, Batch 1500/18331, Loss: 0.1452\n",
      "Epoch 2/3, Batch 1600/18331, Loss: 0.1461\n",
      "Epoch 2/3, Batch 1700/18331, Loss: 0.1449\n",
      "Epoch 2/3, Batch 1800/18331, Loss: 0.1453\n",
      "Epoch 2/3, Batch 1900/18331, Loss: 0.1443\n",
      "Epoch 2/3, Batch 2000/18331, Loss: 0.1441\n",
      "Epoch 2/3, Batch 2100/18331, Loss: 0.1426\n",
      "Epoch 2/3, Batch 2200/18331, Loss: 0.1421\n",
      "Epoch 2/3, Batch 2300/18331, Loss: 0.1425\n",
      "Epoch 2/3, Batch 2400/18331, Loss: 0.1423\n",
      "Epoch 2/3, Batch 2500/18331, Loss: 0.1416\n",
      "Epoch 2/3, Batch 2600/18331, Loss: 0.1411\n",
      "Epoch 2/3, Batch 2700/18331, Loss: 0.1409\n",
      "Epoch 2/3, Batch 2800/18331, Loss: 0.1403\n",
      "Epoch 2/3, Batch 2900/18331, Loss: 0.1400\n",
      "Epoch 2/3, Batch 3000/18331, Loss: 0.1400\n",
      "Epoch 2/3, Batch 3100/18331, Loss: 0.1403\n",
      "Epoch 2/3, Batch 3200/18331, Loss: 0.1400\n",
      "Epoch 2/3, Batch 3300/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 3400/18331, Loss: 0.1397\n",
      "Epoch 2/3, Batch 3500/18331, Loss: 0.1396\n",
      "Epoch 2/3, Batch 3600/18331, Loss: 0.1398\n",
      "Epoch 2/3, Batch 3700/18331, Loss: 0.1403\n",
      "Epoch 2/3, Batch 3800/18331, Loss: 0.1402\n",
      "Epoch 2/3, Batch 3900/18331, Loss: 0.1403\n",
      "Epoch 2/3, Batch 4000/18331, Loss: 0.1402\n",
      "Epoch 2/3, Batch 4100/18331, Loss: 0.1405\n",
      "Epoch 2/3, Batch 4200/18331, Loss: 0.1406\n",
      "Epoch 2/3, Batch 4300/18331, Loss: 0.1406\n",
      "Epoch 2/3, Batch 4400/18331, Loss: 0.1408\n",
      "Epoch 2/3, Batch 4500/18331, Loss: 0.1411\n",
      "Epoch 2/3, Batch 4600/18331, Loss: 0.1412\n",
      "Epoch 2/3, Batch 4700/18331, Loss: 0.1412\n",
      "Epoch 2/3, Batch 4800/18331, Loss: 0.1413\n",
      "Epoch 2/3, Batch 4900/18331, Loss: 0.1412\n",
      "Epoch 2/3, Batch 5000/18331, Loss: 0.1411\n",
      "Epoch 2/3, Batch 5100/18331, Loss: 0.1409\n",
      "Epoch 2/3, Batch 5200/18331, Loss: 0.1407\n",
      "Epoch 2/3, Batch 5300/18331, Loss: 0.1407\n",
      "Epoch 2/3, Batch 5400/18331, Loss: 0.1406\n",
      "Epoch 2/3, Batch 5500/18331, Loss: 0.1404\n",
      "Epoch 2/3, Batch 5600/18331, Loss: 0.1406\n",
      "Epoch 2/3, Batch 5700/18331, Loss: 0.1406\n",
      "Epoch 2/3, Batch 5800/18331, Loss: 0.1404\n",
      "Epoch 2/3, Batch 5900/18331, Loss: 0.1401\n",
      "Epoch 2/3, Batch 6000/18331, Loss: 0.1402\n",
      "Epoch 2/3, Batch 6100/18331, Loss: 0.1407\n",
      "Epoch 2/3, Batch 6200/18331, Loss: 0.1404\n",
      "Epoch 2/3, Batch 6300/18331, Loss: 0.1404\n",
      "Epoch 2/3, Batch 6400/18331, Loss: 0.1403\n",
      "Epoch 2/3, Batch 6500/18331, Loss: 0.1401\n",
      "Epoch 2/3, Batch 6600/18331, Loss: 0.1401\n",
      "Epoch 2/3, Batch 6700/18331, Loss: 0.1401\n",
      "Epoch 2/3, Batch 6800/18331, Loss: 0.1401\n",
      "Epoch 2/3, Batch 6900/18331, Loss: 0.1403\n",
      "Epoch 2/3, Batch 7000/18331, Loss: 0.1402\n",
      "Epoch 2/3, Batch 7100/18331, Loss: 0.1400\n",
      "Epoch 2/3, Batch 7200/18331, Loss: 0.1396\n",
      "Epoch 2/3, Batch 7300/18331, Loss: 0.1397\n",
      "Epoch 2/3, Batch 7400/18331, Loss: 0.1396\n",
      "Epoch 2/3, Batch 7500/18331, Loss: 0.1394\n",
      "Epoch 2/3, Batch 7600/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 7700/18331, Loss: 0.1393\n",
      "Epoch 2/3, Batch 7800/18331, Loss: 0.1391\n",
      "Epoch 2/3, Batch 7900/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 8000/18331, Loss: 0.1391\n",
      "Epoch 2/3, Batch 8100/18331, Loss: 0.1391\n",
      "Epoch 2/3, Batch 8200/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 8300/18331, Loss: 0.1391\n",
      "Epoch 2/3, Batch 8400/18331, Loss: 0.1393\n",
      "Epoch 2/3, Batch 8500/18331, Loss: 0.1393\n",
      "Epoch 2/3, Batch 8600/18331, Loss: 0.1394\n",
      "Epoch 2/3, Batch 8700/18331, Loss: 0.1393\n",
      "Epoch 2/3, Batch 8800/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 8900/18331, Loss: 0.1391\n",
      "Epoch 2/3, Batch 9000/18331, Loss: 0.1394\n",
      "Epoch 2/3, Batch 9100/18331, Loss: 0.1395\n",
      "Epoch 2/3, Batch 9200/18331, Loss: 0.1394\n",
      "Epoch 2/3, Batch 9300/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 9400/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 9500/18331, Loss: 0.1390\n",
      "Epoch 2/3, Batch 9600/18331, Loss: 0.1391\n",
      "Epoch 2/3, Batch 9700/18331, Loss: 0.1393\n",
      "Epoch 2/3, Batch 9800/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 9900/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 10000/18331, Loss: 0.1392\n",
      "Epoch 2/3, Batch 10100/18331, Loss: 0.1390\n",
      "Epoch 2/3, Batch 10200/18331, Loss: 0.1389\n",
      "Epoch 2/3, Batch 10300/18331, Loss: 0.1388\n",
      "Epoch 2/3, Batch 10400/18331, Loss: 0.1387\n",
      "Epoch 2/3, Batch 10500/18331, Loss: 0.1385\n",
      "Epoch 2/3, Batch 10600/18331, Loss: 0.1384\n",
      "Epoch 2/3, Batch 10700/18331, Loss: 0.1382\n",
      "Epoch 2/3, Batch 10800/18331, Loss: 0.1383\n",
      "Epoch 2/3, Batch 10900/18331, Loss: 0.1381\n",
      "Epoch 2/3, Batch 11000/18331, Loss: 0.1380\n",
      "Epoch 2/3, Batch 11100/18331, Loss: 0.1379\n",
      "Epoch 2/3, Batch 11200/18331, Loss: 0.1379\n",
      "Epoch 2/3, Batch 11300/18331, Loss: 0.1378\n",
      "Epoch 2/3, Batch 11400/18331, Loss: 0.1376\n",
      "Epoch 2/3, Batch 11500/18331, Loss: 0.1376\n",
      "Epoch 2/3, Batch 11600/18331, Loss: 0.1375\n",
      "Epoch 2/3, Batch 11700/18331, Loss: 0.1375\n",
      "Epoch 2/3, Batch 11800/18331, Loss: 0.1376\n",
      "Epoch 2/3, Batch 11900/18331, Loss: 0.1375\n",
      "Epoch 2/3, Batch 12000/18331, Loss: 0.1375\n",
      "Epoch 2/3, Batch 12100/18331, Loss: 0.1374\n",
      "Epoch 2/3, Batch 12200/18331, Loss: 0.1374\n",
      "Epoch 2/3, Batch 12300/18331, Loss: 0.1373\n",
      "Epoch 2/3, Batch 12400/18331, Loss: 0.1373\n",
      "Epoch 2/3, Batch 12500/18331, Loss: 0.1372\n",
      "Epoch 2/3, Batch 12600/18331, Loss: 0.1370\n",
      "Epoch 2/3, Batch 12700/18331, Loss: 0.1369\n",
      "Epoch 2/3, Batch 12800/18331, Loss: 0.1369\n",
      "Epoch 2/3, Batch 12900/18331, Loss: 0.1368\n",
      "Epoch 2/3, Batch 13000/18331, Loss: 0.1368\n",
      "Epoch 2/3, Batch 13100/18331, Loss: 0.1366\n",
      "Epoch 2/3, Batch 13200/18331, Loss: 0.1365\n",
      "Epoch 2/3, Batch 13300/18331, Loss: 0.1364\n",
      "Epoch 2/3, Batch 13400/18331, Loss: 0.1364\n",
      "Epoch 2/3, Batch 13500/18331, Loss: 0.1363\n",
      "Epoch 2/3, Batch 13600/18331, Loss: 0.1363\n",
      "Epoch 2/3, Batch 13700/18331, Loss: 0.1363\n",
      "Epoch 2/3, Batch 13800/18331, Loss: 0.1363\n",
      "Epoch 2/3, Batch 13900/18331, Loss: 0.1361\n",
      "Epoch 2/3, Batch 14000/18331, Loss: 0.1361\n",
      "Epoch 2/3, Batch 14100/18331, Loss: 0.1359\n",
      "Epoch 2/3, Batch 14200/18331, Loss: 0.1359\n",
      "Epoch 2/3, Batch 14300/18331, Loss: 0.1360\n",
      "Epoch 2/3, Batch 14400/18331, Loss: 0.1359\n",
      "Epoch 2/3, Batch 14500/18331, Loss: 0.1357\n",
      "Epoch 2/3, Batch 14600/18331, Loss: 0.1356\n",
      "Epoch 2/3, Batch 14700/18331, Loss: 0.1356\n",
      "Epoch 2/3, Batch 14800/18331, Loss: 0.1356\n",
      "Epoch 2/3, Batch 14900/18331, Loss: 0.1354\n",
      "Epoch 2/3, Batch 15000/18331, Loss: 0.1354\n",
      "Epoch 2/3, Batch 15100/18331, Loss: 0.1354\n",
      "Epoch 2/3, Batch 15200/18331, Loss: 0.1354\n",
      "Epoch 2/3, Batch 15300/18331, Loss: 0.1355\n",
      "Epoch 2/3, Batch 15400/18331, Loss: 0.1353\n",
      "Epoch 2/3, Batch 15500/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 15600/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 15700/18331, Loss: 0.1351\n",
      "Epoch 2/3, Batch 15800/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 15900/18331, Loss: 0.1351\n",
      "Epoch 2/3, Batch 16000/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 16100/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 16200/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 16300/18331, Loss: 0.1353\n",
      "Epoch 2/3, Batch 16400/18331, Loss: 0.1353\n",
      "Epoch 2/3, Batch 16500/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 16600/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 16700/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 16800/18331, Loss: 0.1351\n",
      "Epoch 2/3, Batch 16900/18331, Loss: 0.1350\n",
      "Epoch 2/3, Batch 17000/18331, Loss: 0.1350\n",
      "Epoch 2/3, Batch 17100/18331, Loss: 0.1351\n",
      "Epoch 2/3, Batch 17200/18331, Loss: 0.1351\n",
      "Epoch 2/3, Batch 17300/18331, Loss: 0.1352\n",
      "Epoch 2/3, Batch 17400/18331, Loss: 0.1350\n",
      "Epoch 2/3, Batch 17500/18331, Loss: 0.1351\n",
      "Epoch 2/3, Batch 17600/18331, Loss: 0.1350\n",
      "Epoch 2/3, Batch 17700/18331, Loss: 0.1350\n",
      "Epoch 2/3, Batch 17800/18331, Loss: 0.1350\n",
      "Epoch 2/3, Batch 17900/18331, Loss: 0.1348\n",
      "Epoch 2/3, Batch 18000/18331, Loss: 0.1347\n",
      "Epoch 2/3, Batch 18100/18331, Loss: 0.1346\n",
      "Epoch 2/3, Batch 18200/18331, Loss: 0.1346\n",
      "Epoch 2/3, Batch 18300/18331, Loss: 0.1346\n",
      "Epoch 3/3, Batch 100/18331, Loss: 0.1099\n",
      "Epoch 3/3, Batch 200/18331, Loss: 0.1059\n",
      "Epoch 3/3, Batch 300/18331, Loss: 0.1066\n",
      "Epoch 3/3, Batch 400/18331, Loss: 0.1023\n",
      "Epoch 3/3, Batch 500/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 600/18331, Loss: 0.1011\n",
      "Epoch 3/3, Batch 700/18331, Loss: 0.1025\n",
      "Epoch 3/3, Batch 800/18331, Loss: 0.1043\n",
      "Epoch 3/3, Batch 900/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 1000/18331, Loss: 0.1026\n",
      "Epoch 3/3, Batch 1100/18331, Loss: 0.1046\n",
      "Epoch 3/3, Batch 1200/18331, Loss: 0.1030\n",
      "Epoch 3/3, Batch 1300/18331, Loss: 0.1019\n",
      "Epoch 3/3, Batch 1400/18331, Loss: 0.1024\n",
      "Epoch 3/3, Batch 1500/18331, Loss: 0.1023\n",
      "Epoch 3/3, Batch 1600/18331, Loss: 0.1029\n",
      "Epoch 3/3, Batch 1700/18331, Loss: 0.1032\n",
      "Epoch 3/3, Batch 1800/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 1900/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 2000/18331, Loss: 0.1038\n",
      "Epoch 3/3, Batch 2100/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 2200/18331, Loss: 0.1038\n",
      "Epoch 3/3, Batch 2300/18331, Loss: 0.1045\n",
      "Epoch 3/3, Batch 2400/18331, Loss: 0.1047\n",
      "Epoch 3/3, Batch 2500/18331, Loss: 0.1050\n",
      "Epoch 3/3, Batch 2600/18331, Loss: 0.1046\n",
      "Epoch 3/3, Batch 2700/18331, Loss: 0.1048\n",
      "Epoch 3/3, Batch 2800/18331, Loss: 0.1040\n",
      "Epoch 3/3, Batch 2900/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 3000/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 3100/18331, Loss: 0.1032\n",
      "Epoch 3/3, Batch 3200/18331, Loss: 0.1028\n",
      "Epoch 3/3, Batch 3300/18331, Loss: 0.1030\n",
      "Epoch 3/3, Batch 3400/18331, Loss: 0.1030\n",
      "Epoch 3/3, Batch 3500/18331, Loss: 0.1031\n",
      "Epoch 3/3, Batch 3600/18331, Loss: 0.1031\n",
      "Epoch 3/3, Batch 3700/18331, Loss: 0.1029\n",
      "Epoch 3/3, Batch 3800/18331, Loss: 0.1028\n",
      "Epoch 3/3, Batch 3900/18331, Loss: 0.1031\n",
      "Epoch 3/3, Batch 4000/18331, Loss: 0.1028\n",
      "Epoch 3/3, Batch 4100/18331, Loss: 0.1030\n",
      "Epoch 3/3, Batch 4200/18331, Loss: 0.1028\n",
      "Epoch 3/3, Batch 4300/18331, Loss: 0.1028\n",
      "Epoch 3/3, Batch 4400/18331, Loss: 0.1030\n",
      "Epoch 3/3, Batch 4500/18331, Loss: 0.1028\n",
      "Epoch 3/3, Batch 4600/18331, Loss: 0.1029\n",
      "Epoch 3/3, Batch 4700/18331, Loss: 0.1030\n",
      "Epoch 3/3, Batch 4800/18331, Loss: 0.1031\n",
      "Epoch 3/3, Batch 4900/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 5000/18331, Loss: 0.1032\n",
      "Epoch 3/3, Batch 5100/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 5200/18331, Loss: 0.1037\n",
      "Epoch 3/3, Batch 5300/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 5400/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 5500/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 5600/18331, Loss: 0.1032\n",
      "Epoch 3/3, Batch 5700/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 5800/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 5900/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 6000/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 6100/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 6200/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 6300/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 6400/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 6500/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 6600/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 6700/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 6800/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 6900/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 7000/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 7100/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 7200/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 7300/18331, Loss: 0.1037\n",
      "Epoch 3/3, Batch 7400/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 7500/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 7600/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 7700/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 7800/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 7900/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 8000/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 8100/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 8200/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 8300/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 8400/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 8500/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 8600/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 8700/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 8800/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 8900/18331, Loss: 0.1032\n",
      "Epoch 3/3, Batch 9000/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 9100/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 9200/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 9300/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 9400/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 9500/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 9600/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 9700/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 9800/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 9900/18331, Loss: 0.1032\n",
      "Epoch 3/3, Batch 10000/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 10100/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 10200/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 10300/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 10400/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 10500/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 10600/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 10700/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 10800/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 10900/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 11000/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 11100/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 11200/18331, Loss: 0.1037\n",
      "Epoch 3/3, Batch 11300/18331, Loss: 0.1038\n",
      "Epoch 3/3, Batch 11400/18331, Loss: 0.1037\n",
      "Epoch 3/3, Batch 11500/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 11600/18331, Loss: 0.1038\n",
      "Epoch 3/3, Batch 11700/18331, Loss: 0.1038\n",
      "Epoch 3/3, Batch 11800/18331, Loss: 0.1037\n",
      "Epoch 3/3, Batch 11900/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 12000/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 12100/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 12200/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 12300/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 12400/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 12500/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 12600/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 12700/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 12800/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 12900/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 13000/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 13100/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 13200/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 13300/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 13400/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 13500/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 13600/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 13700/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 13800/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 13900/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 14000/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 14100/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 14200/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 14300/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 14400/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 14500/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 14600/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 14700/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 14800/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 14900/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 15000/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 15100/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 15200/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 15300/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 15400/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 15500/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 15600/18331, Loss: 0.1032\n",
      "Epoch 3/3, Batch 15700/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 15800/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 15900/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 16000/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 16100/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 16200/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 16300/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 16400/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 16500/18331, Loss: 0.1036\n",
      "Epoch 3/3, Batch 16600/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 16700/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 16800/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 16900/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 17000/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 17100/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 17200/18331, Loss: 0.1035\n",
      "Epoch 3/3, Batch 17300/18331, Loss: 0.1034\n",
      "Epoch 3/3, Batch 17400/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 17500/18331, Loss: 0.1032\n",
      "Epoch 3/3, Batch 17600/18331, Loss: 0.1031\n",
      "Epoch 3/3, Batch 17700/18331, Loss: 0.1031\n",
      "Epoch 3/3, Batch 17800/18331, Loss: 0.1031\n",
      "Epoch 3/3, Batch 17900/18331, Loss: 0.1032\n",
      "Epoch 3/3, Batch 18000/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 18100/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 18200/18331, Loss: 0.1033\n",
      "Epoch 3/3, Batch 18300/18331, Loss: 0.1032\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "class AlbertSentimentClassifier(nn.Module):\n",
    "    def __init__(self, albert_model):\n",
    "        super(AlbertSentimentClassifier, self).__init__()\n",
    "        self.albert = albert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.albert.config.hidden_size, 2)  # Binary classification: positive or negative\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        outputs = self.albert(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]  # Take the [CLS] token output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the model\n",
    "model = AlbertSentimentClassifier(Model)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "num_epochs = 3  # Example, you can adjust this\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (input_ids, token_type_ids, labels) in enumerate(train_iter):\n",
    "        input_ids, token_type_ids, labels = input_ids.to(device), token_type_ids.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_iter)}, Loss: {total_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77ece9f-201e-4852-8462-5638ac0a2ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'albert_SNLI.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32842ec-044e-47b4-823f-c067ccbcd0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.9501\n",
      "Accuracy on test set: 0.9549\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_iter):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, token_type_ids, labels in test_iter:\n",
    "            input_ids, token_type_ids, labels = input_ids.to(device), token_type_ids.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(input_ids, token_type_ids)\n",
    "            _, predictions = torch.max(logits, 1)\n",
    "\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_iter_ent)\n",
    "evaluate_model(model, test_iter_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5045cab-2157-46c8-b142-8b85ae955bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_all_gpus():\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "               for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "# Define the model architecture\n",
    "class AlbertSentimentClassifier(nn.Module):\n",
    "    def __init__(self, albert_model):\n",
    "        super(AlbertSentimentClassifier, self).__init__()\n",
    "        self.albert = albert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.albert.config.hidden_size, 2)  # Binary classification: positive or negative\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        outputs = self.albert(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]  # Take the [CLS] token output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "device = try_all_gpus()\n",
    "Model = torch.load('albert_SNLI.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14a9a48-ce95-4e2d-a810-24168eeb25a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation location:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1123 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 1123/1123 [00:07<00:00, 144.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trigger tokens state：the accuracy 0.95073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:22<00:00, 49.03it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 117.00it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.48it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 126.91it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.41it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1 rounds of attacking\n",
      "triggers: tensor([6903]) \n",
      "the accuracy :0.94835 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:22<00:00, 49.07it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 119.71it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.50it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.88it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.08it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 2 rounds of attacking\n",
      "triggers: tensor([29499]) \n",
      "the accuracy :0.94360 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:20<00:00, 53.62it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 117.32it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.97it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.44it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 118.24it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 3 rounds of attacking\n",
      "triggers: tensor([29334]) \n",
      "the accuracy :0.94301 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:23<00:00, 47.88it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.39it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.10it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.92it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.20it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 4 rounds of attacking\n",
      "triggers: tensor([29334]) \n",
      "the accuracy :0.94301 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:24<00:00, 45.01it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 116.41it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.07it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 119.96it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.53it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 5 rounds of attacking\n",
      "triggers: tensor([29334]) \n",
      "the accuracy :0.94301 \n",
      "Concatenation location:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:07<00:00, 151.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trigger tokens state：the accuracy 0.94954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:21<00:00, 53.04it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.27it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.05it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 118.84it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.12it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.39it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.96it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.52it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 124.81it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.57it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1 rounds of attacking\n",
      "triggers: tensor([28810, 22641]) \n",
      "the accuracy :0.94776 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:23<00:00, 48.28it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.90it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.30it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.33it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.49it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.25it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.63it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.94it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.18it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.58it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 112.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 2 rounds of attacking\n",
      "triggers: tensor([11660,  8233]) \n",
      "the accuracy :0.91719 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:23<00:00, 48.77it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.92it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.99it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.05it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.53it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.99it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.03it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.34it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.64it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 119.68it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 3 rounds of attacking\n",
      "triggers: tensor([11660,  8233]) \n",
      "the accuracy :0.91719 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:22<00:00, 49.60it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 115.66it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.24it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.61it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.41it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 119.85it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.30it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.14it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 119.21it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.51it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 4 rounds of attacking\n",
      "triggers: tensor([11660,  8233]) \n",
      "the accuracy :0.91719 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:24<00:00, 46.72it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.97it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.16it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.64it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.52it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.62it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 124.99it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.09it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.00it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 118.82it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 5 rounds of attacking\n",
      "triggers: tensor([11660,  8233]) \n",
      "the accuracy :0.91719 \n",
      "Concatenation location:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:07<00:00, 153.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trigger tokens state：the accuracy 0.94895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:22<00:00, 49.14it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 116.63it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.61it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.64it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.86it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.75it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.89it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.69it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.74it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 116.64it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.24it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.38it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 124.95it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.44it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.80it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 119.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1 rounds of attacking\n",
      "triggers: tensor([28810, 22641,  6903]) \n",
      "the accuracy :0.94242 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:21<00:00, 51.85it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.95it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 117.10it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.69it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.32it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.91it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.33it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.48it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.81it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.90it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.80it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 117.44it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.84it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.95it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.80it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 117.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 2 rounds of attacking\n",
      "triggers: tensor([11660, 18948,  7575]) \n",
      "the accuracy :0.90472 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:24<00:00, 45.38it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 117.54it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.30it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.26it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.72it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.57it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.22it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.79it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.33it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.00it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.20it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.40it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 124.95it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.35it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.69it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 3 rounds of attacking\n",
      "triggers: tensor([25653, 18948,  7575]) \n",
      "the accuracy :0.90413 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:23<00:00, 47.82it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 120.56it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.88it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.75it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 118.09it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.45it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.53it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.04it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.53it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.91it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 124.98it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 122.70it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 124.88it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.14it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.07it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 4 rounds of attacking\n",
      "triggers: tensor([25653, 18948, 12000]) \n",
      "the accuracy :0.90057 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1123/1123 [00:24<00:00, 46.57it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 117.06it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 121.99it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.06it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.45it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.63it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.40it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.91it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.69it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 124.81it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 126.22it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 118.74it/s]\n",
      "100%|██████████| 1123/1123 [00:08<00:00, 125.56it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.71it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 123.36it/s]\n",
      "100%|██████████| 1123/1123 [00:09<00:00, 124.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 5 rounds of attacking\n",
      "triggers: tensor([25653,  4248, 12000]) \n",
      "the accuracy :0.88127 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([25653,  4248, 12000]), tensor(0.8813, device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "### Trigger Token\n",
    "\n",
    "def init_trigger_tokens(trigger, num_trigger_tokens):\n",
    "    # Initialize trigger tokens, we use 'the' as initial trigger token\n",
    "    trigger_token_ids = [0] * num_trigger_tokens  # 1996 means 'the'\n",
    "    trigger_token_tensor = torch.tensor(trigger_token_ids)\n",
    "    return trigger_token_tensor\n",
    "\n",
    "\n",
    "def evaluate(net, test_iter, trigger_token_tensor):\n",
    "    # evaluate the accuracy of the model after concatenating the initial trigger token\n",
    "    net = net.to(device[0])\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    n = torch.tensor([0] * len(trigger_token_tensor))\n",
    "    m = deepcopy(trigger_token_tensor)\n",
    "    m = m.unsqueeze(0)\n",
    "    n = n.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            a, b, y = batch\n",
    "            a = torch.cat((a[:, :position], m.repeat_interleave(a.shape[0], dim=0), a[:, position:]), dim=1)\n",
    "            b = torch.cat((b[:, :position], n.repeat_interleave(b.shape[0], dim=0), b[:, position:]), dim=1)\n",
    "            a = a.to(device[0])\n",
    "            b = b.to(device[0])\n",
    "            y = y.to(device[0])\n",
    "            # outputs = net(input_ids=a, token_type_ids=b)\n",
    "            # acc = (outputs.logits.argmax(dim=-1) == y).float().mean()\n",
    "            logits = net(input_ids = a, token_type_ids = b)\n",
    "            acc = (logits.argmax(dim=-1) == y).float().mean()\n",
    "            valid_accs.append(acc)\n",
    "    valid_acc = sum(valid_accs) / len(test_iter)\n",
    "    return valid_acc\n",
    "\n",
    "def extract_grad_hook(net, grad_in, grad_out):  # store the gradient in extracted_grads\n",
    "    extracted_grads.append(grad_out[0].mean(dim=0))\n",
    "\n",
    "\n",
    "def add_hook(net):\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            hook = module.register_backward_hook(extract_grad_hook)\n",
    "            break\n",
    "    return hook\n",
    "\n",
    "\n",
    "def get_gradient(net, test_iter, trigger_token_tensor):  # Calculate the loss to get the gradient\n",
    "    net = net.to(device[0])\n",
    "    net.train()\n",
    "    m = deepcopy(trigger_token_tensor)\n",
    "    m = m.unsqueeze(0)\n",
    "    n = torch.tensor([0] * len(trigger_token_tensor))\n",
    "    n = n.unsqueeze(0)\n",
    "    optimizer = torch.optim.AdamW(net.parameters())\n",
    "    for batch in tqdm(test_iter):\n",
    "        a, b, y = batch\n",
    "        a = torch.cat((a[:, :position], m.repeat_interleave(a.shape[0], dim=0), a[:, position:]), dim=1)\n",
    "        b = torch.cat((b[:, :position], n.repeat_interleave(b.shape[0], dim=0), b[:, position:]), dim=1)\n",
    "        a = a.to(device[0])\n",
    "        b = b.to(device[0])\n",
    "        y = y.to(device[0])\n",
    "        '''\n",
    "        outputs = net(input_ids=a, token_type_ids=b)\n",
    "        l = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        '''\n",
    "        logits = net(input_ids = a, token_type_ids = b)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "def process_gradient(length, num_trigger_tokens):  # Process the gradient to get the average gradient\n",
    "    extracted_grads_copy = extracted_grads\n",
    "    extracted_grads_copy[0] = extracted_grads_copy[0]\n",
    "    temp = extracted_grads_copy[0]\n",
    "    temp = temp.unsqueeze(0)\n",
    "    for i in range(1, length - 1):\n",
    "        extracted_grads_copy[i] = extracted_grads_copy[i]\n",
    "        extracted_grads_copy[i] = extracted_grads_copy[i].unsqueeze(0)\n",
    "        temp = torch.cat((temp, extracted_grads_copy[i]), dim=0)\n",
    "    average_grad = temp.mean(dim=0)[position:position + num_trigger_tokens]\n",
    "    return average_grad\n",
    "\n",
    "\n",
    "def hotflip_attack(averaged_grad, embedding_matrix,\n",
    "                   num_candidates=1, increase_loss=False):\n",
    "    averaged_grad = averaged_grad.cpu()\n",
    "    embedding_matrix = embedding_matrix.cpu()\n",
    "    averaged_grad = averaged_grad.unsqueeze(0)\n",
    "    gradient_dot_embedding_matrix = torch.einsum(\"bij,kj->bik\",\n",
    "                                                 (averaged_grad, embedding_matrix))\n",
    "    if not increase_loss:\n",
    "        gradient_dot_embedding_matrix *= -1\n",
    "        # lower versus increase the class probability.\n",
    "    if num_candidates > 1:  # get top k options\n",
    "        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)\n",
    "        return best_k_ids.detach().cpu().numpy()[0]  # Return candidates\n",
    "    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)\n",
    "    return best_at_each_step[0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def collection_attack(net, test_iter, num_candidates, num_epoch, trigger='the',  # Summarize each function\n",
    "                      num_trigger_tokens=3):\n",
    "    trigger_token_tensor = init_trigger_tokens(trigger, num_trigger_tokens)\n",
    "    print(f'Concatenation location:{position}')\n",
    "    valid_acc = evaluate(net, test_iter, trigger_token_tensor)\n",
    "    print(f'Initial trigger tokens state：the accuracy {valid_acc:.5f}')\n",
    "    embedding_weight = get_embedding_weight(net)\n",
    "    for i in range(num_epoch):\n",
    "        extracted_grads.clear()\n",
    "        hook = add_hook(net)\n",
    "        get_gradient(net, test_iter, trigger_token_tensor)\n",
    "        hook.remove()\n",
    "        average_grad = process_gradient(len(test_iter), num_trigger_tokens)\n",
    "        hot_token = hotflip_attack(average_grad, embedding_weight, num_candidates, increase_loss=True)\n",
    "        hot_token_tensor = torch.from_numpy(hot_token)\n",
    "        trigger_token_tensor, valid_acc = select_best_candid(net, test_iter, hot_token_tensor, trigger_token_tensor,\n",
    "                                                             valid_acc)\n",
    "        print(f'after {i + 1} rounds of attacking\\ntriggers: {trigger_token_tensor} \\nthe accuracy :{valid_acc:.5f} ')\n",
    "    return trigger_token_tensor, valid_acc  # Return the final trigger tokens (trigger length) and the accuracy after the attack\n",
    "\n",
    "\n",
    "def get_embedding_weight(net):\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            weight = module.weight\n",
    "            break\n",
    "    return weight\n",
    "\n",
    "\n",
    "def select_best_candid(net, test_iter, candid_trigger, trigger_token, valid_acc):\n",
    "    # Concatenate each candidate to each input to determine the final trigger token\n",
    "    n = torch.tensor([0] * len(trigger_token))\n",
    "    n = n.unsqueeze(0)\n",
    "    trigger_token = trigger_token.unsqueeze(0)\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    for i in range(candid_trigger.shape[0]):\n",
    "        trigger_token_temp = deepcopy(trigger_token)\n",
    "        for j in range(candid_trigger.shape[1]):\n",
    "            trigger_token_temp[0, i] = candid_trigger[i, j]\n",
    "            valid_accs = []\n",
    "            for batch in tqdm(test_iter):\n",
    "                a, b, y = batch\n",
    "                a = torch.cat((a[:, :position], trigger_token_temp.repeat_interleave(a.shape[0], dim=0),\n",
    "                               a[:, position:]), dim=1)\n",
    "                b = torch.cat((b[:, :position], n.repeat_interleave(b.shape[0], dim=0),\n",
    "                               b[:, position:]), dim=1)\n",
    "                a = a.to(device[0])\n",
    "                b = b.to(device[0])\n",
    "                y = y.to(device[0])\n",
    "                #outputs = net(input_ids=a, token_type_ids=b)\n",
    "                #acc = (outputs.logits.argmax(dim=-1) == y).float().mean()\n",
    "                logits = net(input_ids = a, token_type_ids = b)\n",
    "                acc = (logits.argmax(dim=-1) == y).float().mean()\n",
    "                valid_accs.append(acc)\n",
    "            temp = sum(valid_accs) / len(test_iter)\n",
    "            if temp < valid_acc:\n",
    "                valid_acc = temp\n",
    "                trigger_token[0, i] = candid_trigger[i, j]\n",
    "    return trigger_token[0], valid_acc  # Return the final trigger token and the accuracy after the attack\n",
    "\n",
    "collection_attack(Model, test_iter_ent, 5, 5, trigger='<pad>', num_trigger_tokens=1)\n",
    "collection_attack(Model, test_iter_ent, 5, 5, trigger='<pad>', num_trigger_tokens=2)\n",
    "collection_attack(Model, test_iter_ent, 5, 5, trigger='<pad>', num_trigger_tokens=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3435da61-ceff-43ec-b4bb-6206d30645ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation location:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:07<00:00, 146.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trigger tokens state：the accuracy 0.95335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:23<00:00, 45.28it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 116.60it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.13it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.47it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.08it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1 rounds of attacking\n",
      "triggers: tensor([23690]) \n",
      "the accuracy :0.94223 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:22<00:00, 47.15it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 112.56it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.31it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.52it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 118.42it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 2 rounds of attacking\n",
      "triggers: tensor([23690]) \n",
      "the accuracy :0.94223 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:23<00:00, 46.44it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 118.00it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.79it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 116.82it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.43it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 118.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 3 rounds of attacking\n",
      "triggers: tensor([23690]) \n",
      "the accuracy :0.94223 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:21<00:00, 50.77it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 117.91it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.67it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.64it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.10it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 4 rounds of attacking\n",
      "triggers: tensor([23690]) \n",
      "the accuracy :0.94223 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:21<00:00, 50.35it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 120.74it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.18it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.11it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 116.03it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 5 rounds of attacking\n",
      "triggers: tensor([23690]) \n",
      "the accuracy :0.94223 \n",
      "Concatenation location:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:07<00:00, 151.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trigger tokens state：the accuracy 0.95273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:22<00:00, 48.56it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 118.84it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.23it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.88it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.28it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 121.09it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.52it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.65it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.52it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.62it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1 rounds of attacking\n",
      "triggers: tensor([    0, 29833]) \n",
      "the accuracy :0.95088 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:22<00:00, 48.75it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 121.04it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.79it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 121.47it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.42it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.12it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.12it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.96it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 120.01it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.73it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 2 rounds of attacking\n",
      "triggers: tensor([28439, 26987]) \n",
      "the accuracy :0.93636 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:21<00:00, 51.22it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 116.80it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.32it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.56it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.96it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.99it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.45it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.69it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.66it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.90it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 118.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 3 rounds of attacking\n",
      "triggers: tensor([28439, 26987]) \n",
      "the accuracy :0.93636 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:22<00:00, 48.28it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 116.95it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.20it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.91it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 119.76it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.66it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.26it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.15it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.01it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 121.70it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 4 rounds of attacking\n",
      "triggers: tensor([28439, 26987]) \n",
      "the accuracy :0.93636 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:23<00:00, 45.53it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 117.82it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.38it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 120.21it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.73it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.42it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.35it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.71it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 120.94it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.49it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 5 rounds of attacking\n",
      "triggers: tensor([28439, 26987]) \n",
      "the accuracy :0.93636 \n",
      "Concatenation location:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:06<00:00, 155.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trigger tokens state：the accuracy 0.95181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:23<00:00, 46.71it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.06it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.82it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.47it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.67it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.34it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.08it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.12it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.18it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.39it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.93it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.08it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.32it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.40it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.25it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1 rounds of attacking\n",
      "triggers: tensor([26340,     0, 25994]) \n",
      "the accuracy :0.94995 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:23<00:00, 45.46it/s]\n",
      "100%|██████████| 1079/1079 [00:09<00:00, 118.30it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 120.78it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.85it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.13it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.28it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.27it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.47it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.53it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.38it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.05it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.29it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.10it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.13it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.33it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 121.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 2 rounds of attacking\n",
      "triggers: tensor([29143, 29993, 16309]) \n",
      "the accuracy :0.93111 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:22<00:00, 48.41it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.22it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.10it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.87it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.88it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.64it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.56it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.18it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.08it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.77it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.19it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.13it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.49it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 121.65it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.25it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 3 rounds of attacking\n",
      "triggers: tensor([22616, 29993, 16309]) \n",
      "the accuracy :0.92462 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:21<00:00, 49.43it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.60it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.87it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.48it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.33it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.32it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.02it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 120.21it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.74it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 120.00it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.12it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 121.08it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.98it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 121.04it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.61it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 4 rounds of attacking\n",
      "triggers: tensor([22616, 29993, 16309]) \n",
      "the accuracy :0.92462 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:22<00:00, 48.87it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.82it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.39it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.57it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.41it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.88it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.10it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.91it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.75it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.01it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 125.03it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 121.13it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.27it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 122.27it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 124.87it/s]\n",
      "100%|██████████| 1079/1079 [00:08<00:00, 123.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 5 rounds of attacking\n",
      "triggers: tensor([22616, 29993, 16309]) \n",
      "the accuracy :0.92462 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([22616, 29993, 16309]), tensor(0.9246, device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_attack(Model, test_iter_con, 5, 5, trigger='<pad>', num_trigger_tokens=1)\n",
    "collection_attack(Model, test_iter_con, 5, 5, trigger='<pad>', num_trigger_tokens=2)\n",
    "collection_attack(Model, test_iter_con, 5, 5, trigger='<pad>', num_trigger_tokens=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1406a-555a-4668-8756-03cc16e8a246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
