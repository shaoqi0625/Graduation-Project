{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaabc8f-db3d-4460-82f4-87f2bd5fc3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "import warnings\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "global extracted_grads\n",
    "\n",
    "extracted_grads = []\n",
    "position = 1  # concatenation position\n",
    "# the concatenation position of the BERT model is after the [CLS] token\n",
    "# Random Concatenation Mode\n",
    "# position = random.randint(1,500)\n",
    "\n",
    "tokenize = AlbertTokenizer.from_pretrained(\"/root/albert\")\n",
    "Model = AlbertModel.from_pretrained(\"/root/albert\")\n",
    "\n",
    "\n",
    "# Load model related information\n",
    "\n",
    "# Print the number of Total Parameters\n",
    "# total = [param.nelement() for param in Model.parameters()]\n",
    "# print(f'total parameters:{format(sum(total))}\\n each layer parameters{total} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b08c0a2-471f-4d39-9fb0-5b623601b8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: This is a good example.\n",
      "Adversarial sentence: This exist a estimable lesson .\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def textfooler(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged_tokens):\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            continue\n",
    "        \n",
    "        synonyms = generate_synonyms(word)\n",
    "        if len(synonyms) > 0:\n",
    "            # Choose a random synonym as replacement\n",
    "            new_word = synonyms[0]\n",
    "            tokens[i] = new_word\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "original_sentence = \"This is a good example.\"\n",
    "adversarial_sentence = textfooler(original_sentence)\n",
    "print(\"Original sentence:\", original_sentence)\n",
    "print(\"Adversarial sentence:\", adversarial_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e95126-9153-421f-a1e5-543f2489f1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_iter):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, token_type_ids, labels in test_iter:\n",
    "            input_ids, token_type_ids, labels = input_ids.to(device), token_type_ids.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(input_ids, token_type_ids)\n",
    "            _, predictions = torch.max(logits, 1)\n",
    "\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Accuracy on test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8565da-6205-43cc-be48-1c75eadd6ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_all_gpus():\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "               for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "# Define the model architecture\n",
    "class AlbertSentimentClassifier(nn.Module):\n",
    "    def __init__(self, albert_model):\n",
    "        super(AlbertSentimentClassifier, self).__init__()\n",
    "        self.albert = albert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.albert.config.hidden_size, 2)  # Binary classification: positive or negative\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        outputs = self.albert(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]  # Take the [CLS] token output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "device = try_all_gpus()\n",
    "Model = torch.load('albert_SNLI.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e8dcb6a-b384-471f-afd6-eb7578685196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data1(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data2(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        text = \"fertilizer bathroom vacuum \" + text\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "\n",
    "def load_snli_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Constructs a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def load_snli_data1(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data1('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter\n",
    "\n",
    "\n",
    "def load_snli_data2(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data2('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be6b0cc-c4c5-40cf-92a8-7e55da01da0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter1 = load_snli_data1(3)\n",
    "test_iter2 = load_snli_data2(3)\n",
    "print(\"reading data finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a800188c-51b7-4b56-ac75-d4d187d4485b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8602\n",
      "Accuracy on test set: 0.8088\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(Model, test_iter1)\n",
    "evaluate_model(Model, test_iter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268b2132-8c10-4b69-857f-baa59ebaa641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data1(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data2(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        text = \"fuhrer tyrion survives \" + text\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "\n",
    "def load_snli_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Constructs a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def load_snli_data1(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data1('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter\n",
    "\n",
    "\n",
    "def load_snli_data2(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data2('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938ea419-2de5-4c02-80e7-a4991a5fdbdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter1 = load_snli_data1(3)\n",
    "test_iter2 = load_snli_data2(3)\n",
    "print(\"reading data finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f406c567-5a83-41ca-9039-3ba8e58b5aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.7210\n",
      "Accuracy on test set: 0.6030\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(Model, test_iter1)\n",
    "evaluate_model(Model, test_iter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a63b9-b68d-478b-95f5-8556302afc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe94512-5f82-4e7e-a3dd-16e750a84ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data1(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data2(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        text = \"wrestling person wrestle \" + text\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "\n",
    "def load_snli_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Constructs a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def load_snli_data1(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data1('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter\n",
    "\n",
    "\n",
    "def load_snli_data2(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data2('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66507bec-2e73-4d29-8fc7-a8a720e492f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter1 = load_snli_data1(3)\n",
    "test_iter2 = load_snli_data2(3)\n",
    "print(\"reading data finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e966b37d-b82a-415c-87f3-d1cb2da19c58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8890\n",
      "Accuracy on test set: 0.8836\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(Model, test_iter1)\n",
    "evaluate_model(Model, test_iter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a88094-dc30-45d7-8fa3-569df2315038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac37f161-13e7-4afc-a05f-6d0b40bbba3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n",
      "Accuracy on test set: 0.6781\n",
      "Accuracy on test set: 0.6290\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data1(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data2(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        text = \"##lder greyhound catching \" + text\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "\n",
    "def load_snli_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Constructs a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def load_snli_data1(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data1('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter\n",
    "\n",
    "\n",
    "def load_snli_data2(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data2('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter\n",
    "\n",
    "\n",
    "test_iter1 = load_snli_data1(3)\n",
    "test_iter2 = load_snli_data2(3)\n",
    "print(\"reading data finished\\n\")\n",
    "\n",
    "\n",
    "evaluate_model(Model, test_iter1)\n",
    "evaluate_model(Model, test_iter2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
