{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85da969c-e500-48c5-b734-14b1b93dd5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import warnings\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "global extracted_grads\n",
    "\n",
    "extracted_grads = []\n",
    "position = 1  # concatenation position\n",
    "# the concatenation position of the BERT model is after the [CLS] token\n",
    "# Random Concatenation Mode\n",
    "# position = random.randint(1,500)\n",
    "\n",
    "tokenize = RobertaTokenizer.from_pretrained(\"/root/roberta\")\n",
    "model = RobertaModel.from_pretrained(\"/root/roberta\")\n",
    "\n",
    "\n",
    "# Load model related information\n",
    "\n",
    "# Print the number of Total Parameters\n",
    "# total = [param.nelement() for param in Model.parameters()]\n",
    "# print(f'total parameters:{format(sum(total))}\\n each layer parameters{total} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256db9b4-4075-447b-bad9-b544dc6f9ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: This is a good example.\n",
      "Adversarial sentence: This represent a ripe representative .\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def textfooler(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged_tokens):\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            continue\n",
    "        \n",
    "        synonyms = generate_synonyms(word)\n",
    "        if len(synonyms) > 0:\n",
    "            # Choose a random synonym as replacement\n",
    "            new_word = synonyms[0]\n",
    "            tokens[i] = new_word\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "original_sentence = \"This is a good example.\"\n",
    "adversarial_sentence = textfooler(original_sentence)\n",
    "print(\"Original sentence:\", original_sentence)\n",
    "print(\"Adversarial sentence:\", adversarial_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e942e7d6-3c8b-456c-827b-a30d4a330ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_all_gpus():\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "               for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "device = try_all_gpus()\n",
    "Model = torch.load('roberta_IMDB.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6397c201-7e86-4331-823c-784e4d868161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# 测试模型\n",
    "def test_model(model, test_loader, criterion):\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with tqdm(test_loader, unit=\"batch\") as ttest:\n",
    "        ttest.set_description(f\"Testing\")\n",
    "\n",
    "        for input_ids, attention_mask, labels in ttest:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device[0]), attention_mask.to(device[0]), labels.to(device[0])\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            ttest.set_postfix(loss=loss.item())\n",
    "\n",
    "    loss = running_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    test_losses.append(loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a534d488-8bae-4177-a23d-7e4e87575ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "\n",
    "def read_test_data1(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'pos'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            # review = 'miserable what entertaining ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def read_test_data2(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'pos'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            review = 'sucked overall . ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_imdb_data1(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    test_data = read_test_data1(data_dir, False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def load_imdb_data2(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    test_data = read_test_data2(data_dir, False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "test_iter1 = load_imdb_data1(10)\n",
    "print(\"reading data finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "569a63e8-f30a-47e0-96af-d222a59be77f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1250/1250 [00:54<00:00, 22.90batch/s, loss=0.576]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5328, Test Accuracy: 80.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses1, test_accuracies1 = test_model(Model, test_iter1, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "004e9a57-5da9-441e-8419-f1988d0fa23e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "\n",
    "def read_test_data1(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'neg'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            # review = 'miserable what entertaining ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def read_test_data2(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'neg'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            review = 'sucked overall . ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_imdb_data1(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    test_data = read_test_data1(data_dir, False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def load_imdb_data2(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    test_data = read_test_data2(data_dir, False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "test_iter2 = load_imdb_data1(10)\n",
    "print(\"reading data finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27b540be-59a2-4a33-9aef-3cd3164d05d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1250/1250 [00:54<00:00, 23.02batch/s, loss=0.871]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0057, Test Accuracy: 16.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses1, test_accuracies1 = test_model(Model, test_iter2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fc949-6f67-4881-b8dd-b578b48a9192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62442209-b6b1-48d8-95f5-5b1b0bbd6834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "\n",
    "def read_test_data1(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'pos'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            # review = 'miserable what entertaining ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def read_test_data2(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'pos'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            review = 'sucked overall . ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_imdb_data1(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    test_data = read_test_data1(data_dir, False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def load_imdb_data2(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    test_data = read_test_data2(data_dir, False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "test_iter1 = load_imdb_data1(10)\n",
    "test_iter2 = load_imdb_data2(10)\n",
    "print(\"reading data finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d1f31c9-ec22-4343-af47-8ffa73b1cbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1250/1250 [00:53<00:00, 23.30batch/s, loss=0.031] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5552, Test Accuracy: 80.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1250/1250 [00:54<00:00, 23.10batch/s, loss=0.621]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7723, Test Accuracy: 48.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses1, test_accuracies1 = test_model(Model, test_iter1, criterion)\n",
    "test_losses2, test_accuracies2 = test_model(Model, test_iter2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3edc387-ccef-4291-b97e-c0fd5f9ac1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1250/1250 [00:53<00:00, 23.37batch/s, loss=0.303]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1957, Test Accuracy: 92.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1250/1250 [00:53<00:00, 23.15batch/s, loss=0.721] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5970, Test Accuracy: 74.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "\n",
    "def read_test_data1(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'neg'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            # review = 'miserable what entertaining ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def read_test_data2(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'neg'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            review = 'enjoyable overall overall ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_imdb_data1(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    test_data = read_test_data1(data_dir, False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def load_imdb_data2(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    test_data = read_test_data2(data_dir, False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "test_iter1 = load_imdb_data1(10)\n",
    "test_iter2 = load_imdb_data2(10)\n",
    "print(\"reading data finished\")\n",
    "\n",
    "test_losses1, test_accuracies1 = test_model(Model, test_iter1, criterion)\n",
    "test_losses2, test_accuracies2 = test_model(Model, test_iter2, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
