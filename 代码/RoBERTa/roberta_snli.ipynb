{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d532a1d-a4c9-4070-be1b-7745cad7879f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import warnings\n",
    "import csv\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "global extracted_grads\n",
    "\n",
    "extracted_grads = []\n",
    "position = 1  # concatenation position\n",
    "# the concatenation position of the BERT model is after the [CLS] token\n",
    "# Random Concatenation Mode\n",
    "# position = random.randint(1,500)\n",
    "\n",
    "tokenize = RobertaTokenizer.from_pretrained(\"/root/roberta\")\n",
    "model = RobertaModel.from_pretrained(\"/root/roberta\")\n",
    "\n",
    "# Load model related information\n",
    "\n",
    "# Print the number of Total Parameters\n",
    "# total = [param.nelement() for param in Model.parameters()]\n",
    "# print(f'total parameters:{format(sum(total))}\\n each layer parameters{total} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c556838-8751-4284-953f-f5b19f6b6cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_data(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data_ent(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    return texts, labels\n",
    "\n",
    "def read_snli_binary_test_data_con(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def load_snli_data_ent(train_batch_size, test_batch_size, num_steps=500):\n",
    "    train_data = read_snli_binary_data('snli_1.0', is_train=True)\n",
    "    test_data = read_snli_binary_test_data_ent('snli_1.0', is_train=False)\n",
    "\n",
    "    train_encoding = tokenize(train_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    train_labels = torch.tensor(train_data[1])\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    train_dataset = data.TensorDataset(train_encoding['input_ids'], train_encoding['attention_mask'], train_labels)\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, train_batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def load_snli_data_con(train_batch_size, test_batch_size, num_steps=500):\n",
    "    train_data = read_snli_binary_data('snli_1.0', is_train=True)\n",
    "    test_data = read_snli_binary_test_data_con('snli_1.0', is_train=False)\n",
    "\n",
    "    train_encoding = tokenize(train_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    train_labels = torch.tensor(train_data[1])\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    train_dataset = data.TensorDataset(train_encoding['input_ids'], train_encoding['attention_mask'], train_labels)\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, train_batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bcca905-5d44-47b3-8731-e3a1d420f63f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n",
      "18331\n",
      "1123\n",
      "1079\n"
     ]
    }
   ],
   "source": [
    "# train_iter, test_iter = load_imdb_data(10)\n",
    "# train_iter, test_iter = load_sst_data(10)\n",
    "train_iter, test_iter_ent = load_snli_data_ent(20, 3)\n",
    "train_iter, test_iter_con = load_snli_data_con(20, 3)\n",
    "# Data preprocessing and loading\n",
    "print(\"reading data finished\\n\")\n",
    "print(len(train_iter))\n",
    "print(len(test_iter_ent))\n",
    "print(len(test_iter_con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f668d1ed-bd85-4ee0-86ef-4c553c595c89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 18331/18331 [18:15<00:00, 16.73batch/s, loss=0.254]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.1265, Accuracy: 95.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 18331/18331 [18:14<00:00, 16.74batch/s, loss=0.024]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.0796, Accuracy: 97.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 18331/18331 [18:05<00:00, 16.89batch/s, loss=0.00257] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0589, Accuracy: 97.92%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.12650721268822113, 0.0795707771738208, 0.058891038116733005],\n",
       " [95.25672184897559, 97.17105424669192, 97.92473056685297])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设定随机种子，以确保实验结果可复现\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义模型\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "model = SentimentClassifier(num_classes=2).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=3):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            for input_ids, attention_mask, labels in tepoch:\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions * 100\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_iter, criterion, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24076806-959a-4584-bf02-1875fbd62f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"roberta_snli.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cbf732-fb19-4ed5-bf00-c908b113b01f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1123/1123 [00:09<00:00, 120.86batch/s, loss=0.0016] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0726, Test Accuracy: 97.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1079/1079 [00:08<00:00, 120.25batch/s, loss=0.000342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0616, Test Accuracy: 97.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "def test_model(model, test_loader, criterion):\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with tqdm(test_loader, unit=\"batch\") as ttest:\n",
    "        ttest.set_description(f\"Testing\")\n",
    "\n",
    "        for input_ids, attention_mask, labels in ttest:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            ttest.set_postfix(loss=loss.item())\n",
    "\n",
    "    loss = running_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    test_losses.append(loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return test_losses, test_accuracies\n",
    "\n",
    "# 测试模型\n",
    "test_losses_pos, test_accuracies_pos = test_model(model, test_iter_ent, criterion)\n",
    "test_losses_neg, test_accuracies_neg = test_model(model, test_iter_con, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7907f26-a67c-4216-884c-64703a6f6f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a9d98-dfe1-4f23-bb3a-b6d5876a0567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1374a31-1f88-431f-9da5-96f73bd5d885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
