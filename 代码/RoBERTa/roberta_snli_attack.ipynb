{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9b1d7a-a835-4464-ba37-949274c4c175",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import warnings\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "global extracted_grads\n",
    "\n",
    "extracted_grads = []\n",
    "position = 1  # concatenation position\n",
    "# the concatenation position of the BERT model is after the [CLS] token\n",
    "# Random Concatenation Mode\n",
    "# position = random.randint(1,500)\n",
    "\n",
    "tokenize = RobertaTokenizer.from_pretrained(\"/root/roberta\")\n",
    "Model = RobertaModel.from_pretrained(\"/root/roberta\")\n",
    "\n",
    "\n",
    "# Load model related information\n",
    "\n",
    "# Print the number of Total Parameters\n",
    "# total = [param.nelement() for param in Model.parameters()]\n",
    "# print(f'total parameters:{format(sum(total))}\\n each layer parameters{total} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3345b29-07a0-448b-91a2-ef332c70cd23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: This is a good example.\n",
      "Adversarial sentence: This live a sound model .\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def textfooler(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged_tokens):\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            continue\n",
    "        \n",
    "        synonyms = generate_synonyms(word)\n",
    "        if len(synonyms) > 0:\n",
    "            # Choose a random synonym as replacement\n",
    "            new_word = synonyms[0]\n",
    "            tokens[i] = new_word\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "original_sentence = \"This is a good example.\"\n",
    "adversarial_sentence = textfooler(original_sentence)\n",
    "print(\"Original sentence:\", original_sentence)\n",
    "print(\"Adversarial sentence:\", adversarial_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b4ffce-1089-4bee-96ea-2b4b76b6c128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_all_gpus():\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "               for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "device = try_all_gpus()\n",
    "Model = torch.load('roberta_snli.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f5f28b-02b2-42e5-8f0f-59f7960cd0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# 测试模型\n",
    "def test_model(model, test_loader, criterion):\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with tqdm(test_loader, unit=\"batch\") as ttest:\n",
    "        ttest.set_description(f\"Testing\")\n",
    "\n",
    "        for input_ids, attention_mask, labels in ttest:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device[0]), attention_mask.to(device[0]), labels.to(device[0])\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            ttest.set_postfix(loss=loss.item())\n",
    "\n",
    "    loss = running_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    test_losses.append(loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1d792c-6373-483e-bae2-d5677023aefb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data1(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data2(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        text = \"wrestling person wrestle \" + text\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def load_snli_data1(test_batch_size, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data1('snli_1.0', is_train=False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def load_snli_data2(test_batch_size, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data2('snli_1.0', is_train=False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fcd825-5e7d-4b67-963e-28c23604e3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter1 = load_snli_data1(3)\n",
    "print(\"reading data finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74172eca-6189-46fa-bcc1-0819383a9156",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1123/1123 [00:09<00:00, 116.69batch/s, loss=0.0178] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4214, Test Accuracy: 82.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses1, test_accuracies1 = test_model(Model, test_iter1, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e3ba27-6b0f-4e43-95de-e16526d1b680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data1(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data2(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        text = \"wrestling person wrestle \" + text\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def load_snli_data1(test_batch_size, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data1('snli_1.0', is_train=False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def load_snli_data2(test_batch_size, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data2('snli_1.0', is_train=False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36f44b3-82b4-47c7-bbb7-30fdc31b6968",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter1 = load_snli_data1(3)\n",
    "print(\"reading data finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf34250-b1ee-4e14-99bb-0e9c60ae4e02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1079/1079 [00:08<00:00, 124.77batch/s, loss=0.102]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4487, Test Accuracy: 82.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses1, test_accuracies1 = test_model(Model, test_iter1, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b3280-7aa4-4b2a-91e7-340399f80b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a1d2f-68fd-4f66-b231-53e7c20221ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f712cc-a5a9-4152-b782-4cff40085ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data1(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data2(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    label_set = {'entailment': 0}\n",
    "    # label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        text = \"wrestling person wrestle \" + text\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def load_snli_data1(test_batch_size, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data1('snli_1.0', is_train=False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def load_snli_data2(test_batch_size, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data2('snli_1.0', is_train=False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b02eaad-391c-470c-9a31-7549b0fbd0ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter1 = load_snli_data1(3)\n",
    "test_iter2 = load_snli_data2(3)\n",
    "print(\"reading data finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27959ddd-9dac-494d-8a0b-e3aad18c174a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1123/1123 [00:09<00:00, 120.12batch/s, loss=1.42]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5897, Test Accuracy: 76.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1123/1123 [00:09<00:00, 115.49batch/s, loss=2]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7201, Test Accuracy: 71.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses1, test_accuracies1 = test_model(Model, test_iter1, criterion)\n",
    "test_losses2, test_accuracies2 = test_model(Model, test_iter2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a4a33-b1cf-49e6-adbb-e570531d2a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125ef225-4600-476a-8314-4862ccdd9bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1079/1079 [00:09<00:00, 110.27batch/s, loss=0.139]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3606, Test Accuracy: 85.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1079/1079 [00:09<00:00, 117.05batch/s, loss=0.116]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3538, Test Accuracy: 84.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data1(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data2(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        text = \"##lder greyhound catching \" + text\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def load_snli_data1(test_batch_size, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data1('snli_1.0', is_train=False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def load_snli_data2(test_batch_size, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data2('snli_1.0', is_train=False)\n",
    "\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, test_batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "test_iter1 = load_snli_data1(3)\n",
    "test_iter2 = load_snli_data2(3)\n",
    "print(\"reading data finished\\n\")\n",
    "\n",
    "test_losses1, test_accuracies1 = test_model(Model, test_iter1, criterion)\n",
    "test_losses2, test_accuracies2 = test_model(Model, test_iter2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ee83c-28e6-46c8-adb1-83340f333667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
