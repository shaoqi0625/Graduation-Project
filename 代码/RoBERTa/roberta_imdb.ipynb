{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894203e4-5f39-4427-9f62-e2453a9226ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "global extracted_grads\n",
    "\n",
    "extracted_grads = []\n",
    "position = 1  # concatenation position\n",
    "# the concatenation position of the BERT model is after the [CLS] token\n",
    "# Random Concatenation Mode\n",
    "# position = random.randint(1,500)\n",
    "\n",
    "tokenize = RobertaTokenizer.from_pretrained(\"/root/roberta\")\n",
    "model = RobertaModel.from_pretrained(\"/root/roberta\")\n",
    "\n",
    "\n",
    "# Load model related information\n",
    "\n",
    "# Print the number of Total Parameters\n",
    "# total = [param.nelement() for param in Model.parameters()]\n",
    "# print(f'total parameters:{format(sum(total))}\\n each layer parameters{total} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d75e11-931c-4b69-abb4-b8a5599fe019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "\n",
    "def read_data(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    for label in ('neg', 'pos'):\n",
    "        data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "        for file in os.listdir(data_path):\n",
    "            with open(os.path.join(data_path, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def read_test_data_pos(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'pos'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def read_test_data_neg(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'neg'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_imdb_data_pos(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    train_data = read_data(data_dir, True)\n",
    "    test_data = read_test_data_pos(data_dir, False)\n",
    "\n",
    "    train_encoding = tokenize(train_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    train_labels = torch.tensor(train_data[1])\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    train_dataset = data.TensorDataset(train_encoding['input_ids'], train_encoding['attention_mask'], train_labels)\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def load_imdb_data_neg(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'\n",
    "    train_data = read_data(data_dir, True)\n",
    "    test_data = read_test_data_neg(data_dir, False)\n",
    "\n",
    "    train_encoding = tokenize(train_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "\n",
    "    train_labels = torch.tensor(train_data[1])\n",
    "    test_labels = torch.tensor(test_data[1])\n",
    "\n",
    "    train_dataset = data.TensorDataset(train_encoding['input_ids'], train_encoding['attention_mask'], train_labels)\n",
    "    test_dataset = data.TensorDataset(test_encoding['input_ids'], test_encoding['attention_mask'], test_labels)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "train_loader, test_loader_pos = load_imdb_data_pos(batch_size=10)\n",
    "train_loader, test_loader_neg = load_imdb_data_neg(batch_size=10)\n",
    "print(\"reading data finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ea5c5f-48ef-4e64-b111-ee89d0d05496",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 2500/2500 [05:42<00:00,  7.29batch/s, loss=0.0471] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.2026, Accuracy: 91.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 2500/2500 [05:43<00:00,  7.27batch/s, loss=0.0863] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.1218, Accuracy: 95.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 2500/2500 [05:43<00:00,  7.28batch/s, loss=0.0118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0831, Accuracy: 97.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.202579672957398, 0.12180150676416233, 0.08305188301452436],\n",
       " [91.824, 95.696, 97.208])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设定随机种子，以确保实验结果可复现\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义模型\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "model = SentimentClassifier(num_classes=2).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=3):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            for input_ids, attention_mask, labels in tepoch:\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions * 100\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c004c9fa-9960-4b37-9c07-1c980761484a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'roberta_IMDB.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f9c884-e339-4606-bd04-109da4b56348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1250/1250 [00:52<00:00, 23.84batch/s, loss=0.000889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0622, Test Accuracy: 98.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1250/1250 [00:52<00:00, 23.89batch/s, loss=0.0428]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3126, Test Accuracy: 90.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "def test_model(model, test_loader, criterion):\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with tqdm(test_loader, unit=\"batch\") as ttest:\n",
    "        ttest.set_description(f\"Testing\")\n",
    "\n",
    "        for input_ids, attention_mask, labels in ttest:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            ttest.set_postfix(loss=loss.item())\n",
    "\n",
    "    loss = running_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    test_losses.append(loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return test_losses, test_accuracies\n",
    "\n",
    "# 测试模型\n",
    "test_losses_pos, test_accuracies_pos = test_model(model, test_loader_pos, criterion)\n",
    "test_losses_neg, test_accuracies_neg = test_model(model, test_loader_neg, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac30814-7d12-41d7-b6fe-a1631acfb974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_all_gpus():\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "               for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "device = try_all_gpus()\n",
    "Model = torch.load('roberta_IMDB.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e8d033-86df-4de7-9b04-89d0cde855a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation location:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:49<00:00, 25.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trigger tokens state：the accuracy 0.98048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [02:30<00:00,  8.28it/s]\n",
      "100%|██████████| 1250/1250 [00:50<00:00, 24.99it/s]\n",
      " 41%|████▏     | 518/1250 [00:20<00:29, 24.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 179\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trigger_token[\u001b[38;5;241m0\u001b[39m], valid_acc  \u001b[38;5;66;03m# Return the final trigger token and the accuracy after the attack\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m#collection_attack(Model, test_loader_pos, 5, 5, trigger='<pad>', num_trigger_tokens=1)\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[43mcollection_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<pad>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_trigger_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m#collection_attack(Model, test_loader_pos, 5, 10, trigger='<pad>', num_trigger_tokens=3)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 129\u001b[0m, in \u001b[0;36mcollection_attack\u001b[0;34m(net, test_iter, num_candidates, num_epoch, trigger, num_trigger_tokens)\u001b[0m\n\u001b[1;32m    127\u001b[0m     hot_token \u001b[38;5;241m=\u001b[39m hotflip_attack(average_grad, embedding_weight, num_candidates, increase_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    128\u001b[0m     hot_token_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(hot_token)\n\u001b[0;32m--> 129\u001b[0m     trigger_token_tensor, valid_acc \u001b[38;5;241m=\u001b[39m \u001b[43mselect_best_candid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhot_token_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigger_token_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43mvalid_acc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rounds of attacking\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtriggers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrigger_token_tensor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mthe accuracy :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trigger_token_tensor, valid_acc\n",
      "Cell \u001b[0;32mIn[5], line 161\u001b[0m, in \u001b[0;36mselect_best_candid\u001b[0;34m(net, test_iter, candid_trigger, trigger_token, valid_acc)\u001b[0m\n\u001b[1;32m    157\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((a[:, :position], trigger_token_temp\u001b[38;5;241m.\u001b[39mrepeat_interleave(a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    158\u001b[0m                a[:, position:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    159\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((b[:, :position], n\u001b[38;5;241m.\u001b[39mrepeat_interleave(b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    160\u001b[0m                b[:, position:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m b \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mto(device[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    163\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "### Trigger Token\n",
    "\n",
    "def init_trigger_tokens(trigger, num_trigger_tokens):\n",
    "    # Initialize trigger tokens, we use 'the' as initial trigger token\n",
    "    trigger_token_ids = [0] * num_trigger_tokens  # 1996 means 'the'\n",
    "    trigger_token_tensor = torch.tensor(trigger_token_ids)\n",
    "    return trigger_token_tensor\n",
    "\n",
    "\n",
    "def evaluate(net, test_iter, trigger_token_tensor):\n",
    "    # evaluate the accuracy of the model after concatenating the initial trigger token\n",
    "    net = net.to(device[0])\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    n = torch.tensor([0] * len(trigger_token_tensor))\n",
    "    m = deepcopy(trigger_token_tensor)\n",
    "    m = m.unsqueeze(0)\n",
    "    n = n.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            a, b, y = batch\n",
    "            a = torch.cat((a[:, :position], m.repeat_interleave(a.shape[0], dim=0), a[:, position:]), dim=1)\n",
    "            b = torch.cat((b[:, :position], n.repeat_interleave(b.shape[0], dim=0), b[:, position:]), dim=1)\n",
    "            a = a.to(device[0])\n",
    "            b = b.to(device[0])\n",
    "            y = y.to(device[0])\n",
    "            # outputs = net(input_ids=a, token_type_ids=b)\n",
    "            # acc = (outputs.logits.argmax(dim=-1) == y).float().mean()\n",
    "            # logits = net(input_ids = a, token_type_ids = b)\n",
    "            # acc = (logits.argmax(dim=-1) == y).float().mean()\n",
    "            outputs = net(input_ids=a, attention_mask=b)\n",
    "            acc = (outputs.argmax(dim=-1) == y).float().mean()\n",
    "            \n",
    "            valid_accs.append(acc)\n",
    "    valid_acc = sum(valid_accs) / len(test_iter)\n",
    "    return valid_acc\n",
    "\n",
    "def extract_grad_hook(net, grad_in, grad_out):  # store the gradient in extracted_grads\n",
    "    extracted_grads.append(grad_out[0].mean(dim=0))\n",
    "\n",
    "\n",
    "def add_hook(net):\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            hook = module.register_backward_hook(extract_grad_hook)\n",
    "            break\n",
    "    return hook\n",
    "\n",
    "\n",
    "def get_gradient(net, test_iter, trigger_token_tensor):  # Calculate the loss to get the gradient\n",
    "    net = net.to(device[0])\n",
    "    net.train()\n",
    "    m = deepcopy(trigger_token_tensor)\n",
    "    m = m.unsqueeze(0)\n",
    "    n = torch.tensor([0] * len(trigger_token_tensor))\n",
    "    n = n.unsqueeze(0)\n",
    "    optimizer = torch.optim.AdamW(net.parameters())\n",
    "    for batch in tqdm(test_iter):\n",
    "        a, b, y = batch\n",
    "        a = torch.cat((a[:, :position], m.repeat_interleave(a.shape[0], dim=0), a[:, position:]), dim=1)\n",
    "        b = torch.cat((b[:, :position], n.repeat_interleave(b.shape[0], dim=0), b[:, position:]), dim=1)\n",
    "        a = a.to(device[0])\n",
    "        b = b.to(device[0])\n",
    "        y = y.to(device[0])\n",
    "        '''\n",
    "        outputs = net(input_ids=a, token_type_ids=b)\n",
    "        l = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        '''\n",
    "        '''\n",
    "        logits = net(input_ids = a, token_type_ids = b)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        '''\n",
    "        outputs = net(input_ids = a, attention_mask = b)\n",
    "        loss = criterion(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "def process_gradient(length, num_trigger_tokens):  # Process the gradient to get the average gradient\n",
    "    extracted_grads_copy = extracted_grads\n",
    "    extracted_grads_copy[0] = extracted_grads_copy[0]\n",
    "    temp = extracted_grads_copy[0]\n",
    "    temp = temp.unsqueeze(0)\n",
    "    for i in range(1, length - 1):\n",
    "        extracted_grads_copy[i] = extracted_grads_copy[i]\n",
    "        extracted_grads_copy[i] = extracted_grads_copy[i].unsqueeze(0)\n",
    "        temp = torch.cat((temp, extracted_grads_copy[i]), dim=0)\n",
    "    average_grad = temp.mean(dim=0)[position:position + num_trigger_tokens]\n",
    "    return average_grad\n",
    "\n",
    "\n",
    "def hotflip_attack(averaged_grad, embedding_matrix,\n",
    "                   num_candidates=1, increase_loss=False):\n",
    "    averaged_grad = averaged_grad.cpu()\n",
    "    embedding_matrix = embedding_matrix.cpu()\n",
    "    averaged_grad = averaged_grad.unsqueeze(0)\n",
    "    gradient_dot_embedding_matrix = torch.einsum(\"bij,kj->bik\",\n",
    "                                                 (averaged_grad, embedding_matrix))\n",
    "    if not increase_loss:\n",
    "        gradient_dot_embedding_matrix *= -1\n",
    "        # lower versus increase the class probability.\n",
    "    if num_candidates > 1:  # get top k options\n",
    "        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)\n",
    "        return best_k_ids.detach().cpu().numpy()[0]  # Return candidates\n",
    "    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)\n",
    "    return best_at_each_step[0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def collection_attack(net, test_iter, num_candidates, num_epoch, trigger='the',  # Summarize each function\n",
    "                      num_trigger_tokens=3):\n",
    "    trigger_token_tensor = init_trigger_tokens(trigger, num_trigger_tokens)\n",
    "    print(f'Concatenation location:{position}')\n",
    "    valid_acc = evaluate(net, test_iter, trigger_token_tensor)\n",
    "    print(f'Initial trigger tokens state：the accuracy {valid_acc:.5f}')\n",
    "    embedding_weight = get_embedding_weight(net)\n",
    "    for i in range(num_epoch):\n",
    "        extracted_grads.clear()\n",
    "        hook = add_hook(net)\n",
    "        get_gradient(net, test_iter, trigger_token_tensor)\n",
    "        hook.remove()\n",
    "        average_grad = process_gradient(len(test_iter), num_trigger_tokens)\n",
    "        hot_token = hotflip_attack(average_grad, embedding_weight, num_candidates, increase_loss=True)\n",
    "        hot_token_tensor = torch.from_numpy(hot_token)\n",
    "        trigger_token_tensor, valid_acc = select_best_candid(net, test_iter, hot_token_tensor, trigger_token_tensor,\n",
    "                                                             valid_acc)\n",
    "        print(f'after {i + 1} rounds of attacking\\ntriggers: {trigger_token_tensor} \\nthe accuracy :{valid_acc:.5f} ')\n",
    "    return trigger_token_tensor, valid_acc  # Return the final trigger tokens (trigger length) and the accuracy after the attack\n",
    "\n",
    "\n",
    "def get_embedding_weight(net):\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            weight = module.weight\n",
    "            break\n",
    "    return weight\n",
    "\n",
    "\n",
    "def select_best_candid(net, test_iter, candid_trigger, trigger_token, valid_acc):\n",
    "    # Concatenate each candidate to each input to determine the final trigger token\n",
    "    n = torch.tensor([0] * len(trigger_token))\n",
    "    n = n.unsqueeze(0)\n",
    "    trigger_token = trigger_token.unsqueeze(0)\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    for i in range(candid_trigger.shape[0]):\n",
    "        trigger_token_temp = deepcopy(trigger_token)\n",
    "        for j in range(candid_trigger.shape[1]):\n",
    "            trigger_token_temp[0, i] = candid_trigger[i, j]\n",
    "            valid_accs = []\n",
    "            for batch in tqdm(test_iter):\n",
    "                a, b, y = batch\n",
    "                a = torch.cat((a[:, :position], trigger_token_temp.repeat_interleave(a.shape[0], dim=0),\n",
    "                               a[:, position:]), dim=1)\n",
    "                b = torch.cat((b[:, :position], n.repeat_interleave(b.shape[0], dim=0),\n",
    "                               b[:, position:]), dim=1)\n",
    "                a = a.to(device[0])\n",
    "                b = b.to(device[0])\n",
    "                y = y.to(device[0])\n",
    "                #outputs = net(input_ids=a, token_type_ids=b)\n",
    "                #acc = (outputs.logits.argmax(dim=-1) == y).float().mean()\n",
    "                # logits = net(input_ids = a, token_type_ids = b)\n",
    "                # acc = (logits.argmax(dim=-1) == y).float().mean()\n",
    "                outputs = net(input_ids=a, attention_mask=b)\n",
    "                acc = (outputs.argmax(dim=-1) == y).float().mean()    \n",
    "                \n",
    "                valid_accs.append(acc)\n",
    "            temp = sum(valid_accs) / len(test_iter)\n",
    "            if temp < valid_acc:\n",
    "                valid_acc = temp\n",
    "                trigger_token[0, i] = candid_trigger[i, j]\n",
    "    return trigger_token[0], valid_acc  # Return the final trigger token and the accuracy after the attack\n",
    "\n",
    "#collection_attack(Model, test_loader_pos, 5, 5, trigger='<pad>', num_trigger_tokens=1)\n",
    "collection_attack(Model, test_loader_pos, 5, 5, trigger='<pad>', num_trigger_tokens=2)\n",
    "#collection_attack(Model, test_loader_pos, 5, 10, trigger='<pad>', num_trigger_tokens=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a2074-e42c-4109-b7c6-41eeb7815d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
