{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf3f83c-8706-4e25-8a97-e963a35edd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1988: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at PreTrainedModelBert/pytorch_model.bin and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "import warnings\n",
    "import csv\n",
    "BERT_path = 'PreTrainedModelBert'  # path to bert model\n",
    "tokenize = BertTokenizer.from_pretrained(os.path.join(BERT_path, 'vocab.txt'))\n",
    "model_config = BertConfig.from_pretrained(os.path.join(BERT_path, 'config.json'))\n",
    "Model = BertForSequenceClassification.from_pretrained(os.path.join(BERT_path, 'pytorch_model.bin'), config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ea8337-9c26-4395-b1c8-b1aa3103df81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e16c691-d710-48e3-a54d-bb82b485af4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: This is a good example.\n",
      "Adversarial sentence: This comprise a dependable model .\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def textfooler(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged_tokens):\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            continue\n",
    "        \n",
    "        synonyms = generate_synonyms(word)\n",
    "        if len(synonyms) > 0:\n",
    "            # Choose a random synonym as replacement\n",
    "            new_word = synonyms[0]\n",
    "            tokens[i] = new_word\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "original_sentence = \"This is a good example.\"\n",
    "adversarial_sentence = textfooler(original_sentence)\n",
    "print(\"Original sentence:\", original_sentence)\n",
    "print(\"Adversarial sentence:\", adversarial_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b13e8141-a655-4ed3-8471-b0d1f1b6d2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_no(net, test_iter):\n",
    "    net = net.to(device[0])\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            a, b, y = batch\n",
    "            a = a.to(device[0])\n",
    "            b = b.to(device[0])\n",
    "            y = y.to(device[0])\n",
    "            outputs = net(input_ids=a, token_type_ids=b, labels=y)\n",
    "            acc = (outputs.logits.argmax(dim=-1) == y).float().mean()\n",
    "            valid_accs.append(acc)\n",
    "    valid_acc = sum(valid_accs) / len(test_iter)\n",
    "    print(valid_acc)\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4bb4ee-6399-47fd-849c-1c0e2f54b2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IMDB Data\n",
    "'''\n",
    "\n",
    "### Load data\n",
    "\n",
    "\n",
    "def read_test_data1(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'neg'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            # review = 'miserable what entertaining ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def read_test_data2(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    label = 'neg'  # choose a label to attack\n",
    "    data_path = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "    for file in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, file), 'rb') as f:\n",
    "            review = f.read().decode('utf-8').replace('\\n', ' ')\n",
    "            review = textfooler(review)\n",
    "            review = 'enjoyable overall overall ' + review\n",
    "            data.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Constructs a PyTorch data iterator.\"\"\"\n",
    "    #data_arrays = [torch.tensor(arr) for arr in data_arrays]\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def load_imdb_data1(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'  # Path to download dataset\n",
    "    test_data = read_test_data1(data_dir, False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_array((test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "                           batch_size,\n",
    "                           is_train=False)\n",
    "    return test_iter\n",
    "\n",
    "\n",
    "def load_imdb_data2(batch_size, num_steps=500):\n",
    "    data_dir = 'aclImdb'  # Path to download dataset\n",
    "    test_data = read_test_data2(data_dir, False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_array((test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "                           batch_size,\n",
    "                           is_train=False)\n",
    "    return test_iter\n",
    "\n",
    "\n",
    "def try_all_gpus():\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "               for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "test_iter1 = load_imdb_data1(10)\n",
    "test_iter2 = load_imdb_data2(10)\n",
    "\n",
    "#train_iter, test_iter = load_sst_data(10)\n",
    "# Data preprocessing and loading\n",
    "print(\"reading data finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77a8f29a-cfb4-4e80-8a1a-3757dffbf06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = try_all_gpus()\n",
    "model = torch.load('Bert_IMDB.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9db7a49-8952-4c2f-bc4e-19711b3a5e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:55<00:00, 22.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8954, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:52<00:00, 23.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5702, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5702, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_no(model, test_iter1)\n",
    "evaluate_no(model, test_iter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b08f404-c8da-4f6b-b6e4-e6aa37687687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
