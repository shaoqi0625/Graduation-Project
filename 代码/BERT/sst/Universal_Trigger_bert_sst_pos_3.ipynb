{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43e6b5b-b4af-4fd4-998b-2f25ef7c50ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at PreTrainedModelBert/pytorch_model.bin and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "import warnings\n",
    "import csv\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "global extracted_grads\n",
    "\n",
    "extracted_grads = []\n",
    "position = 1  # concatenation position\n",
    "# the concatenation position of the BERT model is after the [CLS] token\n",
    "# Random Concatenation Mode\n",
    "# position = random.randint(1,500)\n",
    "\n",
    "BERT_path = 'PreTrainedModelBert'  # path to bert model\n",
    "tokenize = BertTokenizer.from_pretrained(os.path.join(BERT_path, 'vocab.txt'))\n",
    "model_config = BertConfig.from_pretrained(os.path.join(BERT_path, 'config.json'))\n",
    "Model = BertForSequenceClassification.from_pretrained(os.path.join(BERT_path, 'pytorch_model.bin'), config=model_config)\n",
    "\n",
    "# Load model related information\n",
    "\n",
    "# Print the number of Total Parameters\n",
    "# total = [param.nelement() for param in Model.parameters()]\n",
    "# print(f'total parameters:{format(sum(total))}\\n each layer parameters{total} ')\n",
    "\n",
    "'''\n",
    "SST-2 Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def read_sst_data(data_dir):\n",
    "    data, labels = [], []\n",
    "    csv.register_dialect('my', delimiter='\\t', quoting=csv.QUOTE_ALL)\n",
    "    with open(data_dir) as tsvfile:\n",
    "        file_list = csv.reader(tsvfile, \"my\")\n",
    "        first = True\n",
    "        for line in file_list:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            data.append(line[1])\n",
    "            labels.append(int(line[0]))\n",
    "    csv.unregister_dialect('my')\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def read_sst_test_data(data_dir):\n",
    "    data, labels = [], []\n",
    "    csv.register_dialect('my', delimiter='\\t', quoting=csv.QUOTE_ALL)\n",
    "    with open(data_dir) as tsvfile:\n",
    "        file_list = csv.reader(tsvfile, \"my\")\n",
    "        first = True\n",
    "        for line in file_list:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            if line[0] == '1':  # pos\n",
    "                data.append(line[1])\n",
    "                labels.append(int(line[0]))\n",
    "    csv.unregister_dialect('my')\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_sst_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Constructs a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def load_sst_data(batch_size, num_steps=500):\n",
    "    train_data = read_sst_data(\"SST-2/train.tsv\")\n",
    "    test_data = read_sst_test_data(\"SST-2/test.tsv\")\n",
    "    train_encoding = tokenize(train_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    train_iter = load_sst_array(\n",
    "        (train_encoding['input_ids'], train_encoding['token_type_ids'], torch.tensor(train_data[1])),\n",
    "        batch_size)\n",
    "    test_iter = load_sst_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        1,\n",
    "        is_train=False)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "def try_all_gpus():\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "               for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "\n",
    "### Train\n",
    "\n",
    "def train(net, train_iter, lr, num_epochs, device):\n",
    "    print('---------------------------start---------------------')\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr)\n",
    "    net = net.to(device[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        print(f' epoch {epoch + 1}')\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        train_length = 0\n",
    "        for batch in tqdm(train_iter):\n",
    "            a, b, y = batch\n",
    "            a = a.to(device[0])\n",
    "            b = b.to(device[0])\n",
    "            y = y.to(device[0])\n",
    "            outputs = net(input_ids=a, token_type_ids=b, labels=y)\n",
    "            logits = outputs.logits\n",
    "            l = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            acc = (logits.argmax(dim=-1) == y).float().mean()\n",
    "            train_losses.append(l)\n",
    "            train_accs.append(acc)\n",
    "            train_length += len(y)\n",
    "        print(\"Learning rate for epoch %d：%f\" % (epoch + 1, optimizer.param_groups[0]['lr']))\n",
    "        train_loss = sum(train_losses) / len(train_iter)\n",
    "        train_acc = sum(train_accs) / len(train_iter)\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{num_epochs:03d} ] loss = {train_loss:.5f}   acc = {train_acc:.5f}\")\n",
    "    print('Training process has finished.')\n",
    "    print('the loss of model {:.3f}'.format(train_loss))\n",
    "\n",
    "\n",
    "def evaluate_no(net, test_iter):\n",
    "    net = net.to(device[0])\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            a, b, y = batch\n",
    "            a = a.to(device[0])\n",
    "            b = b.to(device[0])\n",
    "            y = y.to(device[0])\n",
    "            outputs = net(input_ids=a, token_type_ids=b, labels=y)\n",
    "            acc = (outputs.logits.argmax(dim=-1) == y).float().mean()\n",
    "            valid_accs.append(acc)\n",
    "    valid_acc = sum(valid_accs) / len(test_iter)\n",
    "    return valid_acc\n",
    "\n",
    "\n",
    "### Trigger Token\n",
    "\n",
    "def init_trigger_tokens(trigger, num_trigger_tokens):\n",
    "    # Initialize trigger tokens, we use 'the' as initial trigger token\n",
    "    trigger_token_ids = [1996] * num_trigger_tokens  # 1996 means 'the'\n",
    "    trigger_token_tensor = torch.tensor(trigger_token_ids)\n",
    "    return trigger_token_tensor\n",
    "\n",
    "\n",
    "def evaluate(net, test_iter, trigger_token_tensor):\n",
    "    # evaluate the accuracy of the model after concatenating the initial trigger token\n",
    "    net = net.to(device[0])\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    n = torch.tensor([0] * len(trigger_token_tensor))\n",
    "    m = deepcopy(trigger_token_tensor)\n",
    "    m = m.unsqueeze(0)\n",
    "    n = n.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            a, b, y = batch\n",
    "            a = torch.cat((a[:, :position], m.repeat_interleave(a.shape[0], dim=0), a[:, position:]), dim=1)\n",
    "            b = torch.cat((b[:, :position], n.repeat_interleave(b.shape[0], dim=0), b[:, position:]), dim=1)\n",
    "            a = a.to(device[0])\n",
    "            b = b.to(device[0])\n",
    "            y = y.to(device[0])\n",
    "            outputs = net(input_ids=a, token_type_ids=b, labels=y)\n",
    "            acc = (outputs.logits.argmax(dim=-1) == y).float().mean()\n",
    "            valid_accs.append(acc)\n",
    "    valid_acc = sum(valid_accs) / len(test_iter)\n",
    "    return valid_acc\n",
    "\n",
    "\n",
    "def extract_grad_hook(net, grad_in, grad_out):  # store the gradient in extracted_grads\n",
    "    extracted_grads.append(grad_out[0].mean(dim=0))\n",
    "\n",
    "\n",
    "def add_hook(net):\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            hook = module.register_backward_hook(extract_grad_hook)\n",
    "            break\n",
    "    return hook\n",
    "\n",
    "\n",
    "def get_gradient(net, test_iter, trigger_token_tensor):  # Calculate the loss to get the gradient\n",
    "    net = net.to(device[0])\n",
    "    net.train()\n",
    "    m = deepcopy(trigger_token_tensor)\n",
    "    m = m.unsqueeze(0)\n",
    "    n = torch.tensor([0] * len(trigger_token_tensor))\n",
    "    n = n.unsqueeze(0)\n",
    "    optimizer = torch.optim.AdamW(net.parameters())\n",
    "    for batch in tqdm(test_iter):\n",
    "        a, b, y = batch\n",
    "        a = torch.cat((a[:, :position], m.repeat_interleave(a.shape[0], dim=0), a[:, position:]), dim=1)\n",
    "        b = torch.cat((b[:, :position], n.repeat_interleave(b.shape[0], dim=0), b[:, position:]), dim=1)\n",
    "        a = a.to(device[0])\n",
    "        b = b.to(device[0])\n",
    "        y = y.to(device[0])\n",
    "        outputs = net(input_ids=a, token_type_ids=b, labels=y)\n",
    "        l = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "\n",
    "\n",
    "def process_gradient(length, num_trigger_tokens):  # Process the gradient to get the average gradient\n",
    "    extracted_grads_copy = extracted_grads\n",
    "    extracted_grads_copy[0] = extracted_grads_copy[0]\n",
    "    temp = extracted_grads_copy[0]\n",
    "    temp = temp.unsqueeze(0)\n",
    "    for i in range(1, length - 1):\n",
    "        extracted_grads_copy[i] = extracted_grads_copy[i]\n",
    "        extracted_grads_copy[i] = extracted_grads_copy[i].unsqueeze(0)\n",
    "        temp = torch.cat((temp, extracted_grads_copy[i]), dim=0)\n",
    "    average_grad = temp.mean(dim=0)[position:position + num_trigger_tokens]\n",
    "    return average_grad\n",
    "\n",
    "\n",
    "def hotflip_attack(averaged_grad, embedding_matrix,\n",
    "                   num_candidates=1, increase_loss=False):\n",
    "    averaged_grad = averaged_grad.cpu()\n",
    "    embedding_matrix = embedding_matrix.cpu()\n",
    "    averaged_grad = averaged_grad.unsqueeze(0)\n",
    "    gradient_dot_embedding_matrix = torch.einsum(\"bij,kj->bik\",\n",
    "                                                 (averaged_grad, embedding_matrix))\n",
    "    if not increase_loss:\n",
    "        gradient_dot_embedding_matrix *= -1\n",
    "        # lower versus increase the class probability.\n",
    "    if num_candidates > 1:  # get top k options\n",
    "        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)\n",
    "        return best_k_ids.detach().cpu().numpy()[0]  # Return candidates\n",
    "    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)\n",
    "    return best_at_each_step[0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def collection_attack(net, test_iter, num_candidates, num_epoch, trigger='the',  # Summarize each function\n",
    "                      num_trigger_tokens=3):\n",
    "    trigger_token_tensor = init_trigger_tokens(trigger, num_trigger_tokens)\n",
    "    print(f'Concatenation location:{position}')\n",
    "    valid_acc = evaluate(net, test_iter, trigger_token_tensor)\n",
    "    print(f'Initial trigger tokens state：the accuracy {valid_acc:.5f}')\n",
    "    embedding_weight = get_embedding_weight(net)\n",
    "    for i in range(num_epoch):\n",
    "        extracted_grads.clear()\n",
    "        hook = add_hook(net)\n",
    "        get_gradient(net, test_iter, trigger_token_tensor)\n",
    "        hook.remove()\n",
    "        average_grad = process_gradient(len(test_iter), num_trigger_tokens)\n",
    "        hot_token = hotflip_attack(average_grad, embedding_weight, num_candidates, increase_loss=True)\n",
    "        hot_token_tensor = torch.from_numpy(hot_token)\n",
    "        trigger_token_tensor, valid_acc = select_best_candid(net, test_iter, hot_token_tensor, trigger_token_tensor,\n",
    "                                                             valid_acc)\n",
    "        print(f'after {i + 1} rounds of attacking\\ntriggers: {trigger_token_tensor} \\nthe accuracy :{valid_acc:.5f} ')\n",
    "    return trigger_token_tensor, valid_acc  # Return the final trigger tokens (trigger length) and the accuracy after the attack\n",
    "\n",
    "\n",
    "def get_embedding_weight(net):\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            weight = module.weight\n",
    "            break\n",
    "    return weight\n",
    "\n",
    "\n",
    "def select_best_candid(net, test_iter, candid_trigger, trigger_token, valid_acc):\n",
    "    # Concatenate each candidate to each input to determine the final trigger token\n",
    "    n = torch.tensor([0] * len(trigger_token))\n",
    "    n = n.unsqueeze(0)\n",
    "    trigger_token = trigger_token.unsqueeze(0)\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    for i in range(candid_trigger.shape[0]):\n",
    "        trigger_token_temp = deepcopy(trigger_token)\n",
    "        for j in range(candid_trigger.shape[1]):\n",
    "            trigger_token_temp[0, i] = candid_trigger[i, j]\n",
    "            valid_accs = []\n",
    "            for batch in tqdm(test_iter):\n",
    "                a, b, y = batch\n",
    "                a = torch.cat((a[:, :position], trigger_token_temp.repeat_interleave(a.shape[0], dim=0),\n",
    "                               a[:, position:]), dim=1)\n",
    "                b = torch.cat((b[:, :position], n.repeat_interleave(b.shape[0], dim=0),\n",
    "                               b[:, position:]), dim=1)\n",
    "                a = a.to(device[0])\n",
    "                b = b.to(device[0])\n",
    "                y = y.to(device[0])\n",
    "                outputs = net(input_ids=a, token_type_ids=b, labels=y)\n",
    "                acc = (outputs.logits.argmax(dim=-1) == y).float().mean()\n",
    "                valid_accs.append(acc)\n",
    "            temp = sum(valid_accs) / len(test_iter)\n",
    "            if temp < valid_acc:\n",
    "                valid_acc = temp\n",
    "                trigger_token[0, i] = candid_trigger[i, j]\n",
    "    return trigger_token[0], valid_acc  # Return the final trigger token and the accuracy after the attack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b579706-b4a0-4f2c-90ce-710951fb3913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n",
      "6735\n",
      "909\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = load_sst_data(10)\n",
    "# Data preprocessing and loading\n",
    "print(\"reading data finished\\n\")\n",
    "print(len(train_iter))\n",
    "print(len(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5287880-eaa2-41d2-814d-dc269840b6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------start---------------------\n",
      " epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6735 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 6735/6735 [03:44<00:00, 29.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate for epoch 1：0.000005\n",
      "[ Train | 001/003 ] loss = 0.23149   acc = 0.90634\n",
      " epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6735/6735 [03:51<00:00, 29.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate for epoch 2：0.000005\n",
      "[ Train | 002/003 ] loss = 0.12508   acc = 0.95656\n",
      " epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6735/6735 [03:50<00:00, 29.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate for epoch 3：0.000005\n",
      "[ Train | 003/003 ] loss = 0.08985   acc = 0.96975\n",
      "Training process has finished.\n",
      "the loss of model 0.090\n"
     ]
    }
   ],
   "source": [
    "device = try_all_gpus()\n",
    "# device = [torch.device('cpu')]\n",
    "train(Model, train_iter, 5e-6, 3, device)  # base BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5290ae68-f39c-4f8e-af0c-f05371fba5c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(Model, 'Bert_sst.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd8ee735-e729-414f-8e9e-4230bf243345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 912/912 [00:05<00:00, 181.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9068, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_no(Model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295058ab-dd14-4986-99d7-165035067fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/909 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 909/909 [00:05<00:00, 174.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9560, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = try_all_gpus()\n",
    "model = torch.load('Bert_sst.bin')\n",
    "evaluate_no(model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead4c6f1-3b2e-4d7d-933b-922ca7143269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation location:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:05<00:00, 163.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trigger tokens state：the accuracy 0.94609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:18<00:00, 50.36it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.75it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.70it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.31it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.03it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 138.98it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.60it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.50it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 135.93it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 146.72it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.94it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 143.50it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.39it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.72it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 132.62it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 145.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1 rounds of attacking\n",
      "triggers: tensor([ 2339, 15578, 20892]) \n",
      "the accuracy :0.86909 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:18<00:00, 48.26it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 133.39it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 141.51it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 145.55it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.28it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.61it/s]\n",
      "100%|██████████| 909/909 [00:05<00:00, 152.01it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.85it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.92it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 151.16it/s]\n",
      "100%|██████████| 909/909 [00:05<00:00, 151.56it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 144.92it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 138.36it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.88it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.32it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 143.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 2 rounds of attacking\n",
      "triggers: tensor([26693, 15578,  6912]) \n",
      "the accuracy :0.59956 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:18<00:00, 48.54it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 137.31it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.24it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.70it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.61it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.19it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.48it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.89it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.00it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.00it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 139.52it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.58it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 141.76it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.28it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.30it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 3 rounds of attacking\n",
      "triggers: tensor([26693, 28155, 12097]) \n",
      "the accuracy :0.46205 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:17<00:00, 52.89it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.81it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.97it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.22it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.45it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.44it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 142.82it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 142.17it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 137.91it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 146.77it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 134.39it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 143.28it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 132.32it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 142.75it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 145.99it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 4 rounds of attacking\n",
      "triggers: tensor([26693, 15004, 12097]) \n",
      "the accuracy :0.46095 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:17<00:00, 52.37it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.09it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 142.90it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 139.70it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 139.66it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 139.14it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.17it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.37it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 135.24it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 136.94it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 146.30it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.29it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 145.29it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 143.39it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 132.23it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 139.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 5 rounds of attacking\n",
      "triggers: tensor([28557, 14136, 12097]) \n",
      "the accuracy :0.25853 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:18<00:00, 48.52it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 134.75it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.26it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.85it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.67it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.61it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.21it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.31it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.00it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 139.61it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 136.90it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 138.09it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.22it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.21it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.85it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 6 rounds of attacking\n",
      "triggers: tensor([10231, 14136,  3251]) \n",
      "the accuracy :0.14741 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:18<00:00, 47.93it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 136.66it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 142.48it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.42it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 136.29it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 146.51it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 146.44it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.04it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.77it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.99it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.96it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 145.91it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.06it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.92it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.49it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 7 rounds of attacking\n",
      "triggers: tensor([14777, 14136,  3251]) \n",
      "the accuracy :0.14521 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:19<00:00, 47.53it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 142.19it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 151.39it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 140.87it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 144.95it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.15it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.44it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 145.97it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 145.06it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 144.71it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.02it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.27it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.09it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 146.47it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 146.13it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 146.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 8 rounds of attacking\n",
      "triggers: tensor([ 5236, 14136,  3251]) \n",
      "the accuracy :0.13421 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:17<00:00, 53.45it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 133.26it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 145.65it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 136.37it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.43it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.21it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.50it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 139.74it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 146.52it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 138.74it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 138.65it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.60it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.96it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.85it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.94it/s]\n",
      "100%|██████████| 909/909 [00:05<00:00, 151.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 9 rounds of attacking\n",
      "triggers: tensor([ 5236, 14136,  3251]) \n",
      "the accuracy :0.13421 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:17<00:00, 52.61it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.92it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.96it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.27it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.93it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 144.40it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.58it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 150.61it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 136.26it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 144.22it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.68it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 144.91it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.56it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 149.15it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 147.47it/s]\n",
      "100%|██████████| 909/909 [00:06<00:00, 148.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 10 rounds of attacking\n",
      "triggers: tensor([ 5236, 14136,  3251]) \n",
      "the accuracy :0.13421 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 5236, 14136,  3251]), tensor(0.1342, device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_attack(model, test_iter, 5, 10, trigger='the', num_trigger_tokens=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f19b0d-1cfb-4f79-8e3c-002d7cd7f2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
