{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e435bf78-9e94-4cb1-abb8-a58bb2279a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at PreTrainedModelBert/pytorch_model.bin and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "import warnings\n",
    "import csv\n",
    "import re\n",
    "BERT_path = 'PreTrainedModelBert'  # path to bert model\n",
    "tokenize = BertTokenizer.from_pretrained(os.path.join(BERT_path, 'vocab.txt'))\n",
    "model_config = BertConfig.from_pretrained(os.path.join(BERT_path, 'config.json'))\n",
    "Model = BertForSequenceClassification.from_pretrained(os.path.join(BERT_path, 'pytorch_model.bin'), config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9de6aa-7d3f-4270-9853-bd296f2a193b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: This is a good example.\n",
      "Adversarial sentence: This comprise a unspoiled model .\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def textfooler(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged_tokens):\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            continue\n",
    "        \n",
    "        synonyms = generate_synonyms(word)\n",
    "        if len(synonyms) > 0:\n",
    "            # Choose a random synonym as replacement\n",
    "            new_word = synonyms[0]\n",
    "            tokens[i] = new_word\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "original_sentence = \"This is a good example.\"\n",
    "adversarial_sentence = textfooler(original_sentence)\n",
    "print(\"Original sentence:\", original_sentence)\n",
    "print(\"Adversarial sentence:\", adversarial_sentence)\n",
    "\n",
    "def evaluate_no(net, test_iter):\n",
    "    net = net.to(device[0])\n",
    "    net.eval()\n",
    "    valid_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            a, b, y = batch\n",
    "            a = a.to(device[0])\n",
    "            b = b.to(device[0])\n",
    "            y = y.to(device[0])\n",
    "            outputs = net(input_ids=a, token_type_ids=b, labels=y)\n",
    "            acc = (outputs.logits.argmax(dim=-1) == y).float().mean()\n",
    "            valid_accs.append(acc)\n",
    "    valid_acc = sum(valid_accs) / len(test_iter)\n",
    "    print(valid_acc)\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "578dd3c0-6f6d-49d0-a62d-c84d6f75821b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SNLI Data\n",
    "'''\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "def extract_text(s):\n",
    "    # 移除括号\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # 使用一个空格替换两个以上连续空格\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data1(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def read_snli_binary_test_data2(data_dir, is_train):\n",
    "    \"\"\"读取SNLI二分类数据集\"\"\"\n",
    "    # label_set = {'entailment': 0, 'contradiction': 1}\n",
    "    # label_set = {'entailment': 0}\n",
    "    label_set = {'contradiction': 1}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 过滤数据并重新标记标签\n",
    "    data = [(extract_text(row[1]) + ' ' + extract_text(row[2]), label_set[row[0]])\n",
    "            for row in rows if row[0] in label_set]\n",
    "\n",
    "    # 分离文本和标签\n",
    "    texts, labels = zip(*data)\n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        text = textfooler(text)\n",
    "        text = \"##lder greyhound catching \" + text\n",
    "        modified_texts.append(text)\n",
    "\n",
    "    return modified_texts, labels\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "\n",
    "def load_snli_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Constructs a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def load_snli_data1(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data1('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter\n",
    "\n",
    "\n",
    "def load_snli_data2(test_batch_iter, num_steps=500):\n",
    "    test_data = read_snli_binary_test_data2('snli_1.0', is_train=False)\n",
    "    test_encoding = tokenize(test_data[0], return_tensors=\"pt\", padding=True, truncation=True, max_length=num_steps)\n",
    "    test_iter = load_snli_array(\n",
    "        (test_encoding['input_ids'], test_encoding['token_type_ids'], torch.tensor(test_data[1])),\n",
    "        test_batch_iter,\n",
    "        is_train=False)\n",
    "    return test_iter\n",
    "\n",
    "\n",
    "def try_all_gpus():\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "               for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d371b1f2-5ecf-4801-b451-d7cefd8debd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = try_all_gpus()\n",
    "model = torch.load('Bert_snli.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2479a14-80de-47ae-ad99-a02b716317f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_iter1 = load_snli_data1(3)\n",
    "test_iter2 = load_snli_data2(3)\n",
    "print(\"reading data finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78bf56e-1e08-45cf-8b98-15c845d77403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:07<00:00, 151.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8409, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:07<00:00, 149.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7760, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7760, device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_no(model, test_iter1)\n",
    "evaluate_no(model, test_iter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ded2c-4471-4d2b-a026-8441f0bc4fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
